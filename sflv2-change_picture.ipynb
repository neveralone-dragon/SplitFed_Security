{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 一、预处理\n","\n","导入相关函数"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-09T09:22:15.939944Z","iopub.status.busy":"2023-04-09T09:22:15.939011Z","iopub.status.idle":"2023-04-09T09:22:15.950505Z","shell.execute_reply":"2023-04-09T09:22:15.949239Z","shell.execute_reply.started":"2023-04-09T09:22:15.939900Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NVIDIA GeForce GTX 1660 Ti\n"]}],"source":["import time\n","\n","# We have three versions of our implementations\n","# Version1: without using socket and no DP+PixelDP\n","# Version2: with using socket but no DP+PixelDP\n","# Version3: without using socket but with DP+PixelDP\n","\n","# This program is Version1: Single program simulation\n","# ============================================================================\n","import torch\n","from torch import nn\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import math\n","import os.path\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from glob import glob\n","from pandas import DataFrame\n","from collections import OrderedDict\n","\n","\n","import torchvision.datasets as datasets\n","import random\n","import numpy as np\n","import os\n","from torchvision.datasets import ImageFolder\n","\n","import matplotlib\n","\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_colwidth', 1000)\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import copy\n","\n","SEED = 1234\n","\n","def init_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.deterministic = True\n","        print(torch.cuda.get_device_name(0))\n","init_seeds(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.1模型定义"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.157118Z","iopub.status.busy":"2023-04-09T09:22:16.155896Z","iopub.status.idle":"2023-04-09T09:22:16.174878Z","shell.execute_reply":"2023-04-09T09:22:16.173785Z","shell.execute_reply.started":"2023-04-09T09:22:16.157073Z"},"trusted":true},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","class ResNet18_client_side(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(ResNet18_client_side, self).__init__()\n","        self.in_planes = 64\n","        self.conv1 = conv3x3(3, 64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        return out\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet18_server_side(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10, pool_size=4):  # Add a new argument pool_size\n","        super(ResNet18_server_side, self).__init__()\n","        self.in_planes = 64\n","        self.pool_size = pool_size  # Add this line to store pool_size\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.layer2(x)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        # print(\"Output shape before pooling:\", out.shape)\n","        out = F.avg_pool2d(out, kernel_size=self.pool_size)  # Use self.pool_size instead of 8\n","        # print(\"Output shape after pooling:\", out.shape)\n","        out = out.view(out.size(0), -1)\n","        y_hat = self.linear(out)\n","        return y_hat"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 模型初始化"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.374387Z","iopub.status.busy":"2023-04-09T09:22:16.373922Z","iopub.status.idle":"2023-04-09T09:22:16.390977Z","shell.execute_reply":"2023-04-09T09:22:16.389706Z","shell.execute_reply.started":"2023-04-09T09:22:16.374329Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------SFLV2_ResNet18_base----------\n","ResNet18_client_side(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n",")\n"]}],"source":["import logging\n","\n","def init_logging(program_name):\n","    logging.basicConfig(filename=f'{program_name}.log', level=logging.INFO)\n","    logging.getLogger().addHandler(logging.StreamHandler())\n","    \n","# ===================================================================\n","program = \"SFLV2_ResNet18_base\"\n","print(f\"---------{program}----------\")  # this is to identify the program in the slurm outputs files\n","init_logging(program)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# ===================================================================\n","# No. of users\n","num_users = 20\n","epochs = 100\n","frac = 1  # participation of clients; if 1 then 100% clients participate in SFLV2\n","lr = 0.0001\n","\n","# net_glob_client = ResNet18_client_side()\n","net_glob_client = ResNet18_client_side(BasicBlock, [2, 2, 2, 2]).to(device)\n","if torch.cuda.device_count() > 1:\n","    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n","    net_glob_client = nn.DataParallel(\n","        net_glob_client)  # to use the multiple GPUs; later we can change this to CPUs only\n","\n","net_glob_client.to(device)\n","print(net_glob_client)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Server Side Model 服务器端模型"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.394388Z","iopub.status.busy":"2023-04-09T09:22:16.393453Z","iopub.status.idle":"2023-04-09T09:22:16.522326Z","shell.execute_reply":"2023-04-09T09:22:16.520914Z","shell.execute_reply.started":"2023-04-09T09:22:16.394339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ResNet18_server_side(\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}],"source":["# =====================================================================================================\n","#                           Server-side Model definition\n","# =====================================================================================================\n","# Model at server side\n","# Choose dataset: 'HAM10000' or 'CIFAR10'\n","dataset_choice = 'CIFAR10'\n","# Model at server side\n","if dataset_choice == 'HAM10000':\n","    num_classes = 7\n","    pool_size = 8\n","elif dataset_choice == 'CIFAR10':\n","    num_classes = 10\n","    pool_size = 4\n","else:\n","    raise ValueError('Invalid dataset choice.')\n","\n","net_glob_server = ResNet18_server_side(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, pool_size=pool_size)\n","# net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 7) #7 is my numbr of classes\n","if torch.cuda.device_count() > 1:\n","    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n","    net_glob_server = nn.DataParallel(net_glob_server)  # to use the multiple GPUs\n","\n","net_glob_server.to(device)\n","print(net_glob_server)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 变量初始化"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.598584Z","iopub.status.busy":"2023-04-09T09:22:16.598245Z","iopub.status.idle":"2023-04-09T09:22:16.606087Z","shell.execute_reply":"2023-04-09T09:22:16.604922Z","shell.execute_reply.started":"2023-04-09T09:22:16.598552Z"},"trusted":true},"outputs":[],"source":["# ===================================================================================\n","# For Server Side Loss and Accuracy\n","loss_train_collect = []\n","acc_train_collect = []\n","loss_test_collect = []\n","acc_test_collect = []\n","batch_acc_train = []\n","batch_loss_train = []\n","batch_acc_test = []\n","batch_loss_test = []\n","\n","criterion = nn.CrossEntropyLoss()\n","count1 = 0\n","count2 = 0\n","\n","# ====================================================================================================\n","#                                  Server Side Programs\n","# ====================================================================================================\n","# Federated averaging: FedAvg\n","# to print train - test together in each round-- these are made global\n","acc_avg_all_user_train = 0\n","loss_avg_all_user_train = 0\n","loss_train_collect_user = []\n","acc_train_collect_user = []\n","loss_test_collect_user = []\n","acc_test_collect_user = []\n","\n","# client idx collector\n","idx_collect = []\n","l_epoch_check = False\n","fed_check = False"]},{"cell_type":"markdown","metadata":{},"source":["## 1.5 Server端训练定义"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.040140Z","iopub.status.busy":"2023-04-09T09:22:17.039174Z","iopub.status.idle":"2023-04-09T09:22:17.062363Z","shell.execute_reply":"2023-04-09T09:22:17.061299Z","shell.execute_reply.started":"2023-04-09T09:22:17.040091Z"},"trusted":true},"outputs":[],"source":["# Server-side function associated with Training\n","def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n","    \"\"\"\n","\n","    :param fx_client:客户端计算得到的特征值\n","    :param y:对应的标签\n","    :param l_epoch_count:本地训练周期计数\n","    :param l_epoch:本地训练周期总数\n","    :param idx:\n","    :param len_batch:\n","    :return:\n","    \"\"\"\n","    global net_glob_server, criterion, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n","    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect\n","    global loss_train_collect_user, acc_train_collect_user, lr\n","\n","    net_glob_server.train()\n","    optimizer_server = torch.optim.Adam(net_glob_server.parameters(), lr=lr)\n","\n","    # train and update\n","    optimizer_server.zero_grad()\n","\n","    fx_client = fx_client.to(device)\n","    y = y.to(device)\n","\n","    # ---------forward prop-------------\n","    # 将fx_client输入到服务器端模型net_glob_server，计算得到预测值fx_server\n","    fx_server = net_glob_server(fx_client)\n","\n","    # calculate loss\n","    loss = criterion(fx_server, y)\n","    # calculate accuracy\n","    acc = calculate_accuracy(fx_server, y)\n","\n","    # --------backward prop--------------\n","    loss.backward()\n","    # 将fx_client的梯度保存为dfx_client。\n","    dfx_client = fx_client.grad.clone().detach()\n","    optimizer_server.step()\n","\n","    batch_loss_train.append(loss.item())\n","    batch_acc_train.append(acc.item())\n","\n","    # server-side model net_glob_server is global so it is updated automatically in each pass to this function\n","\n","    # count1: to track the completion of the local batch associated with one client\n","    count1 += 1  # 增加计数器count1，用于追踪与一个客户端关联的本地批次的完成情况。\n","    if count1 == len_batch:\n","        # 如果count1等于len_batch，则计算批次的平均损失和准确度，并将它们分别存储在acc_avg_train和loss_avg_train中。然后清空\n","        acc_avg_train = sum(batch_acc_train) / len(batch_acc_train)  # it has accuracy for one batch\n","        loss_avg_train = sum(batch_loss_train) / len(batch_loss_train)\n","\n","        batch_acc_train = []\n","        batch_loss_train = []\n","        count1 = 0\n","\n","        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train,\n","                                                                                      loss_avg_train))\n","\n","        # If one local epoch is completed, after this a new client will come\n","        if l_epoch_count == l_epoch - 1:\n","\n","            l_epoch_check = True  # to evaluate_server function - to check local epoch has completed or not\n","\n","            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n","            # this is because we work on the last trained model and its accuracy (not earlier cases)\n","\n","            # print(\"accuracy = \", acc_avg_train)\n","            acc_avg_train_all = acc_avg_train\n","            loss_avg_train_all = loss_avg_train\n","\n","            # accumulate accuracy and loss for each new user\n","            loss_train_collect_user.append(loss_avg_train_all)\n","            acc_train_collect_user.append(acc_avg_train_all)\n","\n","            # collect the id of each new user\n","            if idx not in idx_collect:\n","                idx_collect.append(idx)\n","                # print(idx_collect)\n","\n","        # This is to check if all users are served for one round --------------------\n","        if len(idx_collect) == num_users * frac:\n","            fed_check = True  # to evaluate_server function  - to check fed check has hitted\n","            # all users served for one round ------------------------- output print and update is done in evaluate_server()\n","            # for nicer display\n","\n","            idx_collect = []\n","\n","            acc_avg_all_user_train = sum(acc_train_collect_user) / len(acc_train_collect_user)\n","            loss_avg_all_user_train = sum(loss_train_collect_user) / len(loss_train_collect_user)\n","\n","            loss_train_collect.append(loss_avg_all_user_train)\n","            acc_train_collect.append(acc_avg_all_user_train)\n","\n","            acc_train_collect_user = []\n","            loss_train_collect_user = []\n","\n","    # send gradients to the client\n","    return dfx_client\n","\n","\n","# Server-side functions associated with Testing\n","def evaluate_server(fx_client, y, idx, len_batch, ell, selected_clients):\n","    global net_glob_server, criterion, batch_acc_test, batch_loss_test\n","    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, l_epoch_check, fed_check\n","    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n","\n","    net_glob_server.eval()\n","    return_local_results = False\n","\n","    with torch.no_grad():\n","        fx_client = fx_client.to(device)\n","        y = y.to(device)\n","        # ---------forward prop-------------\n","        fx_server = net_glob_server(fx_client)\n","\n","        # calculate loss\n","        loss = criterion(fx_server, y)\n","        # calculate accuracy\n","        acc = calculate_accuracy(fx_server, y)\n","\n","        batch_loss_test.append(loss.item())\n","        batch_acc_test.append(acc.item())\n","\n","        # 计数器 count2 用于跟踪处理的客户端数量。如果已经处理了一个批次的客户端数据，那么计算批次的平均损失和平均准确度，然后重置批次损失和批次准确度列表以及计数器。\n","        count2 += 1\n","        if count2 == len_batch:\n","            acc_avg_test = sum(batch_acc_test) / len(batch_acc_test)\n","            loss_avg_test = sum(batch_loss_test) / len(batch_loss_test)\n","\n","            batch_acc_test = []\n","            batch_loss_test = []\n","            count2 = 0\n","\n","            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test,\n","                                                                                             loss_avg_test))\n","\n","            # if a local epoch is completed\n","            if l_epoch_check:\n","                l_epoch_check = False\n","                return_local_results = True\n","\n","                # Store the last accuracy and loss\n","                acc_avg_test_all = acc_avg_test\n","                loss_avg_test_all = loss_avg_test\n","\n","                loss_test_collect_user.append(loss_avg_test_all)\n","                acc_test_collect_user.append(acc_avg_test_all)\n","\n","            # if all users are served for one round ----------\n","            if fed_check:\n","                fed_check = False\n","\n","\n","\n","                # 计算Krum选定客户端的平均准确率和损失\n","                if selected_clients is None or len(selected_clients) == 0:\n","                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n","                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n","                else:\n","                    print(\"选择的客户端index:\",selected_clients)\n","                    acc_test_collect_user = [acc_test_collect_user[i] for i in selected_clients]\n","                    loss_test_collect_user = [loss_test_collect_user[i] for i in selected_clients]\n","\n","                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n","                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n","\n","\n","                loss_test_collect.append(loss_avg_all_user)\n","                acc_test_collect.append(acc_avg_all_user)\n","                acc_test_collect_user = []\n","                loss_test_collect_user = []\n","\n","                print(\"====================== SERVER V1==========================\")\n","                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train,\n","                                                                                          loss_avg_all_user_train))\n","                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user,\n","                                                                                         loss_avg_all_user))\n","                print(\"==========================================================\")\n","\n","    if return_local_results:\n","        return acc_avg_test_all,loss_avg_test_all\n","    else:\n","        return None,None"]},{"cell_type":"markdown","metadata":{},"source":["## 1.6 Client端类定义"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.176954Z","iopub.status.busy":"2023-04-09T09:22:17.176609Z","iopub.status.idle":"2023-04-09T09:22:17.190060Z","shell.execute_reply":"2023-04-09T09:22:17.188999Z","shell.execute_reply.started":"2023-04-09T09:22:17.176923Z"},"trusted":true},"outputs":[],"source":["class Client(object):\n","    def __init__(self, net_client_model, idx, lr, device, dataset_train=None, dataset_test=None, idxs=None,\n","                 idxs_test=None, is_attacker=None, batch_size=128):\n","        \"\"\"\n","        :param idxs: idxs是一个表示该客户端用于训练的数据集的索引列表。在联邦学习中，原始数据集通常由多个客户端持有，每个客户端只能访问自己所持有的部分数据集。因此，为了让每个客户端只使用自己所持有的数据进行训练，需要将原始数据集划分成多个部分，每个部分由一个客户端持有，并通过idxs将该客户端用于训练的数据集的索引列表传递给Client类的构造函数。\n","        :param idxs_test:\n","        \"\"\"\n","        # net_client_model:一个与客户端实例相关的神经网络模型。\n","        self.batch_size = batch_size\n","        self.is_attacker = is_attacker\n","        self.idx = idx  # 一个整数，表示客户端的索引\n","        self.device = device  # 一个字符串，表示执行客户端计算的设备。\n","        self.lr = lr\n","        self.local_ep = 1\n","        # self.selected_clients = []\n","        # DatasetSplit(dataset_train, idxs)表示使用DatasetSplit类将原始的数据集dataset_train按照索引idxs进行划分，以获得当前客户端可用于训练的数据集。\n","        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size=self.batch_size,\n","                                    shuffle=True)\n","        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self, net):\n","        net.train()\n","        optimizer_client = torch.optim.Adam(net.parameters(), lr=self.lr)\n","\n","        for iter in range(self.local_ep):\n","            if self.is_attacker:\n","                dataset_split = self.ldr_train.dataset\n","                num_malicious_samples = len(dataset_split.idxs)\n","                dataset_split.add_malicious_samples(num_malicious_samples)\n","\n","                # Refresh the DataLoader after adding malicious samples\n","                self.ldr_train = DataLoader(dataset_split, batch_size=self.batch_size, shuffle=True)\n","\n","            # 外层循环是客户端的本地训练轮数self.local_ep\n","            len_batch = len(self.ldr_train)  # 计算该客户端的训练集数据分成的批次数。\n","            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n","                # 内层循环是数据加载器self.ldr_train中每个批次的训练。在每个批次中，将图像和标签加载到设备上，然后将优化器的梯度清零。\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                optimizer_client.zero_grad()\n","                # ---------forward prop-------------\n","                fx = net(images)\n","                # 生成一个可求导的副本client_fx\n","                client_fx = fx.clone().detach().requires_grad_(True)\n","\n","                # Sending activations to server and receiving gradients from server\n","                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n","\n","                # --------backward prop -------------\n","                fx.backward(dfx)\n","                optimizer_client.step()\n","\n","            # prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n","\n","        return net.state_dict()\n","\n","    def evaluate(self, net=None, ell=None, selected_clients=None):\n","        net.eval()\n","\n","        with torch.no_grad():\n","            len_batch = len(self.ldr_test)\n","            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                # ---------forward prop-------------\n","                fx = net(images)\n","\n","                # Sending activations to server\n","                acc_avg_test_all, loss_avg_test_all = evaluate_server(fx, labels, self.idx, len_batch, ell,\n","                                                                      selected_clients)\n","\n","            # prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n","            if loss_avg_test_all is not None and acc_avg_test_all is not None:\n","                self.loss_avg_test_all = loss_avg_test_all\n","                self.acc_avg_test_all = acc_avg_test_all\n","\n","        return"]},{"cell_type":"markdown","metadata":{},"source":["## 1.7 聚合算法定义"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.255618Z","iopub.status.busy":"2023-04-09T09:22:17.254634Z","iopub.status.idle":"2023-04-09T09:22:17.263408Z","shell.execute_reply":"2023-04-09T09:22:17.262128Z","shell.execute_reply.started":"2023-04-09T09:22:17.255579Z"},"trusted":true},"outputs":[],"source":["# To print in color -------test/train of the client side\n","def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n","def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n","\n","def calculate_accuracy(fx, y):\n","    preds = fx.max(1, keepdim=True)[1]\n","    correct = preds.eq(y.view_as(preds)).sum()\n","    acc = 100.00 *correct.float()/preds.shape[0]\n","    return acc\n","# Federated averaging: FedAvg\n","def FedAvg(w):\n","    w_avg = copy.deepcopy(w[0])\n","    for k in w_avg.keys():\n","        for i in range(1, len(w)):\n","            w_avg[k] += w[i][k]\n","        w_avg[k] = torch.div(w_avg[k], len(w))\n","    return w_avg"]},{"cell_type":"markdown","metadata":{},"source":["# 2.Data Loading"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 Data处理函数定义"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.672259Z","iopub.status.busy":"2023-04-09T09:22:17.671890Z","iopub.status.idle":"2023-04-09T09:22:17.701920Z","shell.execute_reply":"2023-04-09T09:22:17.700534Z","shell.execute_reply.started":"2023-04-09T09:22:17.672224Z"},"trusted":true},"outputs":[],"source":["from glob import glob\n","from sklearn.model_selection import train_test_split\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class DatasetSplit(Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = list(idxs)\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, label\n","\n","    def add_malicious_samples(self, num_malicious_samples):\n","        for i in range(num_malicious_samples):\n","            # Randomly select an image to modify\n","            dataset_idx = random.choice(self.idxs)\n","            img = self.dataset.data[dataset_idx]\n","\n","            # Check if the image has the shape (3, 32, 32)\n","            if img.shape == (3, 32, 32):\n","                img = np.transpose(img, (1, 2, 0))  # Change the shape to (32, 32, 3)\n","\n","            # Convert the numpy array to a PIL Image\n","            img = Image.fromarray(img)\n","\n","            # Apply a random modification to the image (e.g., flip, rotate, or change color)\n","            # In this example, we flip the image horizontally, rotate, and change the color\n","            modified_img = img.transpose(Image.FLIP_LEFT_RIGHT)  # Flip the image horizontally\n","            modified_img = modified_img.rotate(random.randint(0, 360))  # Rotate the image randomly\n","\n","            # Change the color by adjusting the brightness, contrast, and saturation\n","            brightness = ImageEnhance.Brightness(modified_img)\n","            modified_img = brightness.enhance(random.uniform(0.5, 1.5))\n","\n","            contrast = ImageEnhance.Contrast(modified_img)\n","            modified_img = contrast.enhance(random.uniform(0.5, 1.5))\n","\n","            saturation = ImageEnhance.Color(modified_img)\n","            modified_img = saturation.enhance(random.uniform(0.5, 1.5))\n","\n","            # Convert the modified PIL Image back to a numpy array\n","            modified_img = np.array(modified_img)\n","\n","            # Check if the original image has the shape (3, 32, 32)\n","            if self.dataset.data[dataset_idx].shape == (3, 32, 32):\n","                modified_img = np.transpose(modified_img, (2, 0, 1))  # Change the shape back to (3, 32, 32)\n","\n","            # Update the image in the dataset\n","            self.dataset.data[dataset_idx] = modified_img\n","\n","            # # Change the label of the modified image to a random class\n","            # random_label = random.randint(0, 9)  # Assuming 10 classes in the dataset\n","            # self.dataset.targets[dataset_idx] = random_label\n","\n","class SkinData(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        X = Image.open(self.df['path'][index]).resize((64, 64))\n","        y = torch.tensor(int(self.df['target'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y\n","\n","def dataset_iid(dataset, num_users):\n","    \"\"\"\n","    该函数接受一个数据集dataset和一个整数num_users作为输入。\n","    它的作用是将数据集分割成num_users份，以便每个客户端都有一份相同分布的数据集。\n","    :param dataset:\n","    :param num_users:\n","    :return:函数返回一个字典dict_users，其中包含num_users个键，每个键对应一个客户端，值为该客户端所分配的数据集索引的集合。\n","    dict_users:{idx:int : []:list}\n","    \"\"\"\n","    # 该函数首先计算每个客户端应该拥有的数据量num_items\n","    num_items = int(len(dataset)/num_users)\n","    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n","    for i in range(num_users):\n","        # 接着，函数使用np.random.choice函数从all_idxs中选择num_items个索引，将这些索引添加到字典dict_users的第i个键中，表示第i个客户端的数据集。在选择后，从all_idxs中移除已经分配给第i个客户端的索引。\n","        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n","        all_idxs = list(set(all_idxs) - dict_users[i])\n","    return dict_users\n","\n","def load_data(dataset_choice, num_users):\n","    \"\"\"\n","\n","    :param dataset_choice: 选择的数据集\n","    :param num_users: 用户的数量\n","    :return:\n","    \"\"\"\n","    if dataset_choice == 'HAM10000':\n","        df = pd.read_csv('data/HAM10000_metadata.csv')\n","        print(df.head())\n","\n","        lesion_type = {\n","            'nv': 'Melanocytic nevi',\n","            'mel': 'Melanoma',\n","            'bkl': 'Benign keratosis-like lesions ',\n","            'bcc': 'Basal cell carcinoma',\n","            'akiec': 'Actinic keratoses',\n","            'vasc': 'Vascular lesions',\n","            'df': 'Dermatofibroma'\n","        }\n","\n","        # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n","        imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n","                        for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n","\n","        # print(\"path---------------------------------------\", imageid_path.get)\n","        # 将图像id映射为图像文件的路径，并将其存储在数据集中的path列中。\n","        df['path'] = df['image_id'].map(imageid_path.get)\n","        # 将诊断编码映射为对应的分类名称，并将其存储在数据集中的cell_type列中。\n","        df['cell_type'] = df['dx'].map(lesion_type.get)\n","        # 将分类名称转换为数字编码，并将其存储在数据集中的target列中。这里使用了.\n","        # 可以将字符串类型的分类变量转换为数字编码，其中不同的分类名称对应不同的数字编码。\n","        df['target'] = pd.Categorical(df['cell_type']).codes\n","        print(df['cell_type'].value_counts())\n","        print(df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train, test = train_test_split(df, test_size=0.2)\n","\n","        train = train.reset_index()\n","\n","        test = test.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(64),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(64),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = SkinData(train, transform=train_transforms)\n","        dataset_test = SkinData(test, transform=test_transforms)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    elif dataset_choice == 'CIFAR10':\n","        # =============================================================================\n","        #                         Data loading\n","        # =============================================================================\n","        # Load CIFAR-10 dataset\n","        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n","        testset = datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","        train_df = pd.DataFrame(trainset.targets, columns=['target'])\n","        test_df = pd.DataFrame(testset.targets, columns=['target'])\n","\n","        # Set the class names\n","        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","        train_df['cell_type'] = train_df['target'].apply(lambda x: class_names[x])\n","        test_df['cell_type'] = test_df['target'].apply(lambda x: class_names[x])\n","\n","        print(train_df['cell_type'].value_counts())\n","        print(train_df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train = train_df.reset_index()\n","        test = test_df.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(32),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(32),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n","        dataset_test = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    else:\n","        raise ValueError(\"Invalid dataset_choice: Choose either 'HAM10000' or 'CIFAR10'\")\n","\n","    return dataset_train,dataset_test,dict_users,dict_users_test"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 load data"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.704519Z","iopub.status.busy":"2023-04-09T09:22:17.704050Z","iopub.status.idle":"2023-04-09T09:22:21.052202Z","shell.execute_reply":"2023-04-09T09:22:21.051134Z","shell.execute_reply.started":"2023-04-09T09:22:17.704480Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","frog          5000\n","truck         5000\n","deer          5000\n","automobile    5000\n","bird          5000\n","horse         5000\n","ship          5000\n","cat           5000\n","dog           5000\n","airplane      5000\n","Name: cell_type, dtype: int64\n","6    5000\n","9    5000\n","4    5000\n","1    5000\n","2    5000\n","7    5000\n","8    5000\n","3    5000\n","5    5000\n","0    5000\n","Name: target, dtype: int64\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# =============================================================================\n","#                         Data loading\n","# =============================================================================\n","dataset_train, dataset_test, dict_users, dict_users_test = load_data(dataset_choice, num_users)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2.3数据投毒——标签翻转\n","### 2.3.1函数定义"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:21.054947Z","iopub.status.busy":"2023-04-09T09:22:21.054270Z","iopub.status.idle":"2023-04-09T09:22:21.067110Z","shell.execute_reply":"2023-04-09T09:22:21.066112Z","shell.execute_reply.started":"2023-04-09T09:22:21.054905Z"},"trusted":true},"outputs":[],"source":["def replace_label1_with_label2_on_df(df,label1,label2,poisoned_dict_users):\n","    \"\"\"\n","    标签反转\n","    :param df:dataframe\n","    :param label1: 等待翻转的标签\n","    :param label2: 需要翻转的标签\n","    :param poisoned_dict_users:包含索引列表的字典,中毒的用户\n","    :return:\n","    \"\"\"\n","    for idx_list in poisoned_dict_users.values():\n","        for idx in idx_list:\n","            if df.loc[idx,'target'] == label1:\n","                df.loc[idx,'target'] = label2\n","    # df.loc[df['target'] == label1, 'target'] = label2\n","    return df\n","\n","def random_select_poisoning_users(dict_users, n):\n","    \"\"\"\n","    随机选择n个key-value对\n","    :param dict_users: 字典，key为索引，value为包含索引值的列表\n","    :param n: 选择的key-value对的数量\n","    :return: 随机选择的key-value对组成的字典\n","    \"\"\"\n","    selected = {}\n","    keys = random.sample(list(dict_users.keys()), n)\n","    for k in keys:\n","        selected[k] = dict_users[k]\n","    return selected\n","\n","# def poison_data(dataset_train, dict_users, poisoned_users_num, original_label, target_label):\n","#     poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n","#     replace_label1_with_label2_on_df(dataset_train.df, original_label, target_label, poisoned_dict_users)\n","#     return poisoned_dict_users\n","\n","def print_poisoning_results(poisoned_dict_users, dataset_train):\n","    for poisoned_user_key in poisoned_dict_users:\n","        print(\"被投毒的用户:\", poisoned_user_key)\n","\n","    print(\"标签反转后的target统计:\")\n","    print(dataset_train.df['target'].value_counts())\n","\n","def cifar10_to_dataframe(dataset):\n","    data = [dataset[i] for i in range(len(dataset))]\n","    images, labels = zip(*data)\n","    df = pd.DataFrame({\"image\": images, \"target\": labels})\n","    return df\n","\n","\n","\n","def poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings):\n","    if dataset_choice == 'CIFAR10':\n","        df_train = cifar10_to_dataframe(dataset_train)\n","    elif dataset_choice == 'HAM10000':\n","        df_train = dataset_train.df\n","    else:\n","        raise ValueError(\"Invalid dataset choice.\")\n","\n","    poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n","\n","    for original_label, target_label in label_mappings:\n","        replace_label1_with_label2_on_df(df_train, original_label, target_label, poisoned_dict_users)\n","\n","    # 修改 dataset_train 的标签\n","    if dataset_choice == 'CIFAR10':\n","        for idx, row in df_train.iterrows():\n","            dataset_train.targets[idx] = row['target']\n","\n","    return poisoned_dict_users\n","\n","def load_data(dataset_choice, num_users):\n","    \"\"\"\n","\n","    :param dataset_choice: 选择的数据集\n","    :param num_users: 用户的数量\n","    :return:\n","    \"\"\"\n","    if dataset_choice == 'HAM10000':\n","        df = pd.read_csv('data/HAM10000_metadata.csv')\n","        print(df.head())\n","\n","        lesion_type = {\n","            'nv': 'Melanocytic nevi',\n","            'mel': 'Melanoma',\n","            'bkl': 'Benign keratosis-like lesions ',\n","            'bcc': 'Basal cell carcinoma',\n","            'akiec': 'Actinic keratoses',\n","            'vasc': 'Vascular lesions',\n","            'df': 'Dermatofibroma'\n","        }\n","\n","        # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n","        imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n","                        for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n","\n","        # print(\"path---------------------------------------\", imageid_path.get)\n","        # 将图像id映射为图像文件的路径，并将其存储在数据集中的path列中。\n","        df['path'] = df['image_id'].map(imageid_path.get)\n","        # 将诊断编码映射为对应的分类名称，并将其存储在数据集中的cell_type列中。\n","        df['cell_type'] = df['dx'].map(lesion_type.get)\n","        # 将分类名称转换为数字编码，并将其存储在数据集中的target列中。这里使用了.\n","        # 可以将字符串类型的分类变量转换为数字编码，其中不同的分类名称对应不同的数字编码。\n","        df['target'] = pd.Categorical(df['cell_type']).codes\n","        print(df['cell_type'].value_counts())\n","        print(df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train, test = train_test_split(df, test_size=0.2)\n","\n","        train = train.reset_index()\n","\n","        test = test.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(64),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(64),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = SkinData(train, transform=train_transforms)\n","        dataset_test = SkinData(test, transform=test_transforms)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    elif dataset_choice == 'CIFAR10':\n","        # =============================================================================\n","        #                         Data loading\n","        # =============================================================================\n","        # Load CIFAR-10 dataset\n","        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n","        testset = datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","        train_df = pd.DataFrame(trainset.targets, columns=['target'])\n","        test_df = pd.DataFrame(testset.targets, columns=['target'])\n","\n","        # Set the class names\n","        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","        train_df['cell_type'] = train_df['target'].apply(lambda x: class_names[x])\n","        test_df['cell_type'] = test_df['target'].apply(lambda x: class_names[x])\n","\n","        print(train_df['cell_type'].value_counts())\n","        print(train_df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train = train_df.reset_index()\n","        test = test_df.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(32),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(32),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n","        dataset_test = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    else:\n","        raise ValueError(\"Invalid dataset_choice: Choose either 'HAM10000' or 'CIFAR10'\")\n","\n","    return dataset_train,dataset_test,dict_users,dict_users_test"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:21.070723Z","iopub.status.busy":"2023-04-09T09:22:21.069778Z","iopub.status.idle":"2023-04-09T09:23:27.457519Z","shell.execute_reply":"2023-04-09T09:23:27.456364Z","shell.execute_reply.started":"2023-04-09T09:22:21.070694Z"},"trusted":true},"outputs":[],"source":["def generate_label_mappings(dataset_choice):\n","    if dataset_choice == 'CIFAR10':\n","        num_classes = 10\n","    elif dataset_choice == 'HAM10000':\n","        num_classes = 7\n","    else:\n","        raise ValueError(\"Invalid dataset choice.\")\n","\n","    label_mappings = []\n","    for i in range(num_classes):\n","        target_label = random.choice([j for j in range(num_classes) if j != i])\n","        label_mappings.append((i, target_label))\n","\n","    return label_mappings\n","\n","# =============================================================================\n","#                         Poisoning\n","# =============================================================================\n","# 增加投毒用户数量至 50%\n","# poisoned_frac = 0.5\n","# poisoned_users_num = int(poisoned_frac * num_users)\n","# # label_mappings = [(4, 2), (1, 7), (3, 1), (2, 6), (6, 5)]\n","# label_mappings = generate_label_mappings(dataset_choice)\n","# poisoned_dict_users = poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings)\n","# # poisoned_dict_users = poison_data_random(dataset_train, dataset_choice, dict_users, poisoned_users_num)\n","# # poisoned_dict_users = poison_data_model(dataset_train, dataset_choice, dict_users, poisoned_users_num, attack_pattern, target_label)\n","\n","\n","# for poisoned_user_key in poisoned_dict_users:\n","#     print(\"被投毒的用户:\", poisoned_user_key)\n","\n","# # 如果使用的是 HAM10000 数据集，您可以直接使用 dataset_train.df 查看标签分布\n","# if dataset_choice == 'HAM10000':\n","#     print(\"标签反转后的target统计:\")\n","#     print(dataset_train.df['target'].value_counts())\n","# elif dataset_choice == 'CIFAR10':\n","#     cifar10_df = cifar10_to_dataframe(dataset_train)\n","#     print(\"标签反转后的target统计:\")\n","#     print(cifar10_df['target'].value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["# 3.模型训练和评估"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:23:27.460485Z","iopub.status.busy":"2023-04-09T09:23:27.460129Z","iopub.status.idle":"2023-04-09T09:23:27.468395Z","shell.execute_reply":"2023-04-09T09:23:27.467192Z","shell.execute_reply.started":"2023-04-09T09:23:27.460448Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["匹配攻击选取的客户端 [11  4 14 18  8  1]\n"]}],"source":["# ------------ Training And Testing -----------------\n","net_glob_client.train()\n","# copy weights\n","w_glob_client = net_glob_client.state_dict()\n","\n","# Federation takes place after certain local epochs in train() client-side\n","# this epoch is global epoch, also known as rounds\n","\n","total_time = 0.0  # 初始化总时间为0\n","best_clients_indices = None\n","krum_acc_test_collect = []\n","krum_loss_test_collect = []\n","\n","# 梯度匹配攻击\n","# Choose a fraction of clients to be attackers\n","attackers_frac = 0.3  # Modify this value according to your requirements\n","num_attackers = int(attackers_frac * num_users)\n","attackers_indices = np.random.choice(num_users, num_attackers, replace=False)\n","print(\"匹配攻击选取的客户端\",str(attackers_indices))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:23:27.470769Z","iopub.status.busy":"2023-04-09T09:23:27.470390Z","iopub.status.idle":"2023-04-09T11:13:02.195066Z","shell.execute_reply":"2023-04-09T11:13:02.193866Z","shell.execute_reply.started":"2023-04-09T09:23:27.470731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 22.870 \tLoss: 2.0660\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 7.738 \tLoss: 2.9896\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 29.354 \tLoss: 1.8817\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 9.052 \tLoss: 3.0277\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 35.549 \tLoss: 1.7497\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 9.207 \tLoss: 3.3304\u001b[00m\n","\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 34.924 \tLoss: 1.7169\u001b[00m\n","\u001b[92m Client2 Test =>                   \tAcc: 10.574 \tLoss: 3.1201\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 38.182 \tLoss: 1.6608\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 11.146 \tLoss: 3.1547\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 37.730 \tLoss: 1.6678\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 11.065 \tLoss: 2.7168\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 39.763 \tLoss: 1.6120\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 11.921 \tLoss: 3.2196\u001b[00m\n","\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 42.475 \tLoss: 1.5786\u001b[00m\n","\u001b[92m Client0 Test =>                   \tAcc: 12.197 \tLoss: 3.2391\u001b[00m\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 42.702 \tLoss: 1.5515\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 9.597 \tLoss: 3.6856\u001b[00m\n","\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 43.483 \tLoss: 1.5281\u001b[00m\n","\u001b[92m Client17 Test =>                   \tAcc: 10.163 \tLoss: 3.6123\u001b[00m\n","\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 44.940 \tLoss: 1.4949\u001b[00m\n","\u001b[92m Client9 Test =>                   \tAcc: 11.315 \tLoss: 3.4840\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 46.055 \tLoss: 1.4941\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 14.089 \tLoss: 3.2706\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 47.617 \tLoss: 1.4601\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 14.817 \tLoss: 3.3549\u001b[00m\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 46.494 \tLoss: 1.4615\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 12.042 \tLoss: 4.0915\u001b[00m\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 47.114 \tLoss: 1.4807\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 12.311 \tLoss: 3.8270\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 47.415 \tLoss: 1.4518\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 15.383 \tLoss: 3.8442\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 46.468 \tLoss: 1.4295\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 13.254 \tLoss: 3.5607\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 48.093 \tLoss: 1.4043\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 17.026 \tLoss: 3.2920\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 48.419 \tLoss: 1.4212\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 11.456 \tLoss: 3.4083\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 49.219 \tLoss: 1.3864\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 9.894 \tLoss: 4.5725\u001b[00m\n","====================== SERVER V1==========================\n"," Train: Round   0, Avg Accuracy 41.943 | Avg Loss 1.575\n"," Test: Round   0, Avg Accuracy 11.712 | Avg Loss 3.440\n","==========================================================\n","idxs_users [ 4  7 13  2 14  5 15  0 12 17  9 10 16  8 19 11  6 18  3  1]\n","------------------------------------------------------------\n","------ Fed Server: Federation process at Client-Side -------\n","------------------------------------------------------------\n","Epoch 0 finished in 00:01:30\n","Epoch 0 finished. Total time: 90.99 seconds\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 50.687 \tLoss: 1.3631\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 19.956 \tLoss: 2.5067\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 51.684 \tLoss: 1.3166\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 30.529 \tLoss: 2.2422\u001b[00m\n","\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 51.250 \tLoss: 1.3048\u001b[00m\n","\u001b[92m Client9 Test =>                   \tAcc: 22.400 \tLoss: 2.6436\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 52.188 \tLoss: 1.3268\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 28.320 \tLoss: 2.2812\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 54.968 \tLoss: 1.2875\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 26.933 \tLoss: 2.4925\u001b[00m\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 51.774 \tLoss: 1.3008\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 30.199 \tLoss: 2.3264\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 53.164 \tLoss: 1.2868\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 25.936 \tLoss: 2.5626\u001b[00m\n","\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 53.483 \tLoss: 1.3001\u001b[00m\n","\u001b[92m Client2 Test =>                   \tAcc: 31.452 \tLoss: 2.1128\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 53.819 \tLoss: 1.2847\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 26.172 \tLoss: 2.5815\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 52.645 \tLoss: 1.3060\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 31.897 \tLoss: 2.3120\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 54.630 \tLoss: 1.2516\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 22.326 \tLoss: 2.8969\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 55.469 \tLoss: 1.2231\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 32.072 \tLoss: 2.2083\u001b[00m\n","\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 55.797 \tLoss: 1.2283\u001b[00m\n","\u001b[92m Client17 Test =>                   \tAcc: 26.529 \tLoss: 2.7575\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 55.841 \tLoss: 1.2245\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 31.041 \tLoss: 2.4583\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 54.874 \tLoss: 1.2248\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 29.223 \tLoss: 2.2506\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 55.784 \tLoss: 1.2369\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 31.196 \tLoss: 2.1977\u001b[00m\n","\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 56.553 \tLoss: 1.2059\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 29.647 \tLoss: 2.4351\u001b[00m\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 56.687 \tLoss: 1.2255\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 36.294 \tLoss: 1.8775\u001b[00m\n","\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 54.205 \tLoss: 1.2269\u001b[00m\n","\u001b[92m Client0 Test =>                   \tAcc: 33.661 \tLoss: 2.2233\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 56.349 \tLoss: 1.1951\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 33.324 \tLoss: 2.2040\u001b[00m\n","====================== SERVER V1==========================\n"," Train: Round   1, Avg Accuracy 54.092 | Avg Loss 1.266\n"," Test: Round   1, Avg Accuracy 28.955 | Avg Loss 2.379\n","==========================================================\n","idxs_users [12 16  9 14 10  8 11  2 15  5  3 13 17  1  6  7  4 19  0 18]\n","------------------------------------------------------------\n","------ Fed Server: Federation process at Client-Side -------\n","------------------------------------------------------------\n","Epoch 1 finished in 00:01:24\n","Epoch 1 finished. Total time: 175.57 seconds\n","\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 58.679 \tLoss: 1.1406\u001b[00m\n","\u001b[92m Client0 Test =>                   \tAcc: 53.125 \tLoss: 1.3260\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 59.283 \tLoss: 1.1459\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 47.872 \tLoss: 1.4651\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 60.701 \tLoss: 1.0840\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 49.360 \tLoss: 1.4246\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 61.140 \tLoss: 1.1158\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 53.617 \tLoss: 1.3228\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 60.809 \tLoss: 1.1297\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 50.902 \tLoss: 1.3284\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 58.697 \tLoss: 1.1546\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 55.920 \tLoss: 1.2346\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 58.219 \tLoss: 1.1418\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 55.085 \tLoss: 1.3101\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 57.027 \tLoss: 1.1633\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 55.280 \tLoss: 1.2252\u001b[00m\n","\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 60.046 \tLoss: 1.0926\u001b[00m\n","\u001b[92m Client9 Test =>                   \tAcc: 48.006 \tLoss: 1.5089\u001b[00m\n","\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 59.740 \tLoss: 1.1113\u001b[00m\n","\u001b[92m Client17 Test =>                   \tAcc: 52.223 \tLoss: 1.3753\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 60.788 \tLoss: 1.0882\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 52.034 \tLoss: 1.3666\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 61.487 \tLoss: 1.0915\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 60.203 \tLoss: 1.2180\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 60.342 \tLoss: 1.0930\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 58.641 \tLoss: 1.1081\u001b[00m\n","\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 61.199 \tLoss: 1.0628\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 57.509 \tLoss: 1.1416\u001b[00m\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 60.949 \tLoss: 1.1255\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 56.412 \tLoss: 1.2698\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 60.692 \tLoss: 1.0779\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 50.330 \tLoss: 1.4435\u001b[00m\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 61.356 \tLoss: 1.0844\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 56.061 \tLoss: 1.2871\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 59.437 \tLoss: 1.1144\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 50.364 \tLoss: 1.3793\u001b[00m\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 62.050 \tLoss: 1.0636\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 62.803 \tLoss: 1.0496\u001b[00m\n","\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 62.293 \tLoss: 1.0712\u001b[00m\n","\u001b[92m Client2 Test =>                   \tAcc: 57.738 \tLoss: 1.1557\u001b[00m\n","====================== SERVER V1==========================\n"," Train: Round   2, Avg Accuracy 60.247 | Avg Loss 1.108\n"," Test: Round   2, Avg Accuracy 54.174 | Avg Loss 1.297\n","==========================================================\n","idxs_users [ 0  7 18  1 16  5  6 15  9 17 10  3 13  4 19 11 12 14  8  2]\n","------------------------------------------------------------\n","------ Fed Server: Federation process at Client-Side -------\n","------------------------------------------------------------\n","Epoch 2 finished in 00:01:24\n","Epoch 2 finished. Total time: 259.61 seconds\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 62.932 \tLoss: 1.0441\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 58.904 \tLoss: 1.0993\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 62.594 \tLoss: 1.0608\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 59.557 \tLoss: 1.1045\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 62.976 \tLoss: 1.0226\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 63.194 \tLoss: 1.0826\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 65.136 \tLoss: 0.9884\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 55.785 \tLoss: 1.2840\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 63.088 \tLoss: 0.9995\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 59.382 \tLoss: 1.1224\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 63.201 \tLoss: 1.0266\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 61.160 \tLoss: 1.1611\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 63.874 \tLoss: 1.0428\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 58.917 \tLoss: 1.1921\u001b[00m\n","\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 63.702 \tLoss: 0.9747\u001b[00m\n","\u001b[92m Client9 Test =>                   \tAcc: 57.442 \tLoss: 1.1948\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 64.060 \tLoss: 0.9954\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 58.971 \tLoss: 1.1060\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 64.949 \tLoss: 0.9963\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 58.614 \tLoss: 1.1757\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 64.304 \tLoss: 1.0106\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 60.574 \tLoss: 1.0481\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 65.283 \tLoss: 1.0072\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 65.679 \tLoss: 1.0329\u001b[00m\n","\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 65.951 \tLoss: 0.9663\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 59.597 \tLoss: 1.1549\u001b[00m\n","\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 63.755 \tLoss: 0.9976\u001b[00m\n","\u001b[92m Client2 Test =>                   \tAcc: 58.169 \tLoss: 1.2117\u001b[00m\n","\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 65.809 \tLoss: 0.9679\u001b[00m\n","\u001b[92m Client17 Test =>                   \tAcc: 62.965 \tLoss: 1.0337\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 65.319 \tLoss: 1.0059\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 61.079 \tLoss: 1.0834\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 64.837 \tLoss: 1.0122\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 60.735 \tLoss: 1.1347\u001b[00m\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 64.568 \tLoss: 0.9781\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 58.600 \tLoss: 1.1154\u001b[00m\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 64.573 \tLoss: 0.9715\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 55.900 \tLoss: 1.0947\u001b[00m\n","\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 63.911 \tLoss: 0.9982\u001b[00m\n","\u001b[92m Client0 Test =>                   \tAcc: 63.463 \tLoss: 1.0212\u001b[00m\n","====================== SERVER V1==========================\n"," Train: Round   3, Avg Accuracy 64.241 | Avg Loss 1.003\n"," Test: Round   3, Avg Accuracy 59.934 | Avg Loss 1.123\n","==========================================================\n","idxs_users [19  5  6 18 13 11  7  9 14  1 10  3  4  2 17 16 15 12  8  0]\n","------------------------------------------------------------\n","------ Fed Server: Federation process at Client-Side -------\n","------------------------------------------------------------\n","Epoch 3 finished in 00:01:21\n","Epoch 3 finished. Total time: 341.51 seconds\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 67.898 \tLoss: 0.8976\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 61.880 \tLoss: 1.1508\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 66.312 \tLoss: 0.9559\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 62.513 \tLoss: 1.1248\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 67.158 \tLoss: 0.9236\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 60.964 \tLoss: 1.0524\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 66.181 \tLoss: 0.9140\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 63.759 \tLoss: 1.0553\u001b[00m\n","\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 66.613 \tLoss: 0.9183\u001b[00m\n","\u001b[92m Client9 Test =>                   \tAcc: 67.086 \tLoss: 1.0128\u001b[00m\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 68.148 \tLoss: 0.9064\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 60.810 \tLoss: 1.1218\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 68.139 \tLoss: 0.9308\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 63.254 \tLoss: 1.0506\u001b[00m\n","\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 66.854 \tLoss: 0.9267\u001b[00m\n","\u001b[92m Client2 Test =>                   \tAcc: 64.918 \tLoss: 1.0416\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 68.313 \tLoss: 0.9246\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 65.086 \tLoss: 1.0133\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 67.059 \tLoss: 0.9113\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 56.351 \tLoss: 1.2868\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 68.385 \tLoss: 0.8997\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 63.706 \tLoss: 0.9326\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 66.009 \tLoss: 0.9452\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 66.447 \tLoss: 0.9754\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 67.454 \tLoss: 0.9108\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 56.526 \tLoss: 1.2590\u001b[00m\n","\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 68.617 \tLoss: 0.8856\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 69.093 \tLoss: 0.8624\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 66.854 \tLoss: 0.9211\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 67.652 \tLoss: 0.9550\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 67.631 \tLoss: 0.9166\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 66.474 \tLoss: 0.9960\u001b[00m\n","\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 67.020 \tLoss: 0.9253\u001b[00m\n","\u001b[92m Client0 Test =>                   \tAcc: 64.857 \tLoss: 0.9697\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 67.553 \tLoss: 0.9209\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 65.894 \tLoss: 0.9632\u001b[00m\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 68.270 \tLoss: 0.9020\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 63.746 \tLoss: 1.0861\u001b[00m\n","\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 67.790 \tLoss: 0.9020\u001b[00m\n","\u001b[92m Client17 Test =>                   \tAcc: 63.133 \tLoss: 1.0151\u001b[00m\n","====================== SERVER V1==========================\n"," Train: Round   4, Avg Accuracy 67.413 | Avg Loss 0.917\n"," Test: Round   4, Avg Accuracy 63.708 | Avg Loss 1.046\n","==========================================================\n","idxs_users [12 11  7 13  9  8  1  2  3 18 10  5 15  4 16  6  0 14 19 17]\n","------------------------------------------------------------\n","------ Fed Server: Federation process at Client-Side -------\n","------------------------------------------------------------\n","Epoch 4 finished in 00:01:20\n","Epoch 4 finished. Total time: 421.76 seconds\n","\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 68.938 \tLoss: 0.8534\u001b[00m\n","\u001b[92m Client8 Test =>                   \tAcc: 67.962 \tLoss: 0.8937\u001b[00m\n","\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 68.851 \tLoss: 0.8611\u001b[00m\n","\u001b[92m Client11 Test =>                   \tAcc: 61.045 \tLoss: 1.1410\u001b[00m\n","\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 70.443 \tLoss: 0.8423\u001b[00m\n","\u001b[92m Client12 Test =>                   \tAcc: 67.746 \tLoss: 1.0133\u001b[00m\n","\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 70.053 \tLoss: 0.8410\u001b[00m\n","\u001b[92m Client18 Test =>                   \tAcc: 62.998 \tLoss: 1.0453\u001b[00m\n","\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 69.182 \tLoss: 0.8449\u001b[00m\n","\u001b[92m Client14 Test =>                   \tAcc: 68.312 \tLoss: 0.9338\u001b[00m\n","\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 70.673 \tLoss: 0.8513\u001b[00m\n","\u001b[92m Client4 Test =>                   \tAcc: 65.989 \tLoss: 0.9329\u001b[00m\n","\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 69.345 \tLoss: 0.8553\u001b[00m\n","\u001b[92m Client13 Test =>                   \tAcc: 62.897 \tLoss: 1.0950\u001b[00m\n","\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 69.184 \tLoss: 0.8359\u001b[00m\n","\u001b[92m Client6 Test =>                   \tAcc: 69.195 \tLoss: 0.9079\u001b[00m\n","\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 68.913 \tLoss: 0.8644\u001b[00m\n","\u001b[92m Client3 Test =>                   \tAcc: 68.588 \tLoss: 0.9321\u001b[00m\n","\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 69.111 \tLoss: 0.8775\u001b[00m\n","\u001b[92m Client19 Test =>                   \tAcc: 68.413 \tLoss: 0.9067\u001b[00m\n","\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 70.110 \tLoss: 0.8377\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 63.470 \tLoss: 1.0309\u001b[00m\n","\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 70.843 \tLoss: 0.8367\u001b[00m\n","\u001b[92m Client15 Test =>                   \tAcc: 63.860 \tLoss: 1.0384\u001b[00m\n","\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 69.947 \tLoss: 0.8484\u001b[00m\n","\u001b[92m Client16 Test =>                   \tAcc: 68.777 \tLoss: 0.9716\u001b[00m\n","\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 70.322 \tLoss: 0.8584\u001b[00m\n","\u001b[92m Client7 Test =>                   \tAcc: 67.396 \tLoss: 0.9861\u001b[00m\n","\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 69.970 \tLoss: 0.8273\u001b[00m\n","\u001b[92m Client1 Test =>                   \tAcc: 68.528 \tLoss: 0.8823\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 68.362 \tLoss: 0.8812\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 62.783 \tLoss: 1.0424\u001b[00m\n"]}],"source":["for iter in range(epochs):\n","    start_time = time.time()  # 记录开始时间\n","    m = max(int(frac * num_users), 1)\n","    idxs_users = np.random.choice(range(num_users), m, replace=False)  # ，replace=False表示不允许重复选择。\n","    w_locals_client = []  # 用于存储每个客户端训练后的本地模型参数。\n","    loss_avg_test_all_dict = {}\n","    acc_avg_test_all_dict = {}\n","    idx_local_client_dict = {}\n","\n","    for idx in idxs_users:\n","        is_attacker = idx in attackers_indices\n","        local = Client(net_glob_client, idx, lr, device, dataset_train=dataset_train, dataset_test=dataset_test,\n","                       idxs=dict_users[idx], idxs_test=dict_users_test[idx], is_attacker=is_attacker)\n","        # Training ------------------\n","        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n","        idx_local_client_dict[len(w_locals_client)] = idx\n","        w_locals_client.append(copy.deepcopy(w_client))\n","\n","        # Testing -------------------\n","        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter, selected_clients=best_clients_indices)\n","\n","        # Update the dictionaries with the client's self.loss_avg_test_all and self.acc_avg_test_all\n","        loss_avg_test_all_dict[idx] = local.loss_avg_test_all\n","        acc_avg_test_all_dict[idx] = local.acc_avg_test_all\n","\n","    print(\"idxs_users\", idxs_users)\n","    # Ater serving all clients for its local epochs------------\n","    # Federation process at Client-Side------------------------\n","    print(\"------------------------------------------------------------\")\n","    print(\"------ Fed Server: Federation process at Client-Side -------\")\n","    print(\"------------------------------------------------------------\")\n","    # w_locals_client是所有客户端训练后的本地模型参数列表，FedAvg函数是加权平均函数，返回全局模型参数w_glob_client。\n","    w_glob_client = FedAvg(w_locals_client)\n","    # 调用 Krum 算法\n","    # num_to_select = int(num_users * (1 - poisoned_frac - 0.1))  # 选择的客户端数量\n","    # w_glob_client, best_clients_indices = krum_aggregation(w_locals_client, num_to_select)\n","    # print([idxs_users[i] for i in best_clients_indices])\n","    # Update client-side global model\n","    net_glob_client.load_state_dict(w_glob_client)\n","\n","    # print(\"fedserver选择的客户端index:\", best_clients_indices)\n","    # best_clients_idxs = [idx_local_client_dict[i] for i in best_clients_indices]\n","    # krum_acc_test_collect_user = [acc_avg_test_all_dict[i] for i in best_clients_idxs]\n","    # for acc in krum_acc_test_collect_user:\n","    #     print(\"acc:\",acc)\n","    # krum_loss_test_collect_user = [loss_avg_test_all_dict[i] for i in best_clients_idxs]\n","    #\n","    # krum_acc_avg_all_user = sum(krum_acc_test_collect_user) / len(krum_acc_test_collect_user)\n","    # krum_loss_avg_all_user = sum(krum_loss_test_collect_user) / len(krum_loss_test_collect_user)\n","    # krum_acc_test_collect.append(krum_acc_avg_all_user)\n","    # krum_loss_test_collect.append(krum_loss_avg_all_user)\n","\n","    # print(\"====================== Fed Server==========================\")\n","    # print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, acc_avg_all_user_train,\n","    #                                                                           loss_avg_all_user_train))\n","    # print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, krum_acc_avg_all_user,\n","    #                                                                          krum_loss_avg_all_user))\n","    # print(\"==========================================================\")\n","\n","    end_time = time.time()  # 记录结束时间\n","    epoch_time = end_time - start_time  # 计算epoch所耗费的时间\n","    total_time += epoch_time  # 将时间差加到总时间中\n","    # 将时间差值转换为小时、分钟和秒数\n","    hours, rem = divmod(epoch_time, 3600)\n","    minutes, seconds = divmod(rem, 60)\n","    print(f\"Epoch {iter} finished in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n","    print(f\"Epoch {iter} finished. Total time: {total_time:.2f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T11:13:02.197214Z","iopub.status.busy":"2023-04-09T11:13:02.196807Z","iopub.status.idle":"2023-04-09T11:13:02.227737Z","shell.execute_reply":"2023-04-09T11:13:02.226605Z","shell.execute_reply.started":"2023-04-09T11:13:02.197175Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training and Evaluation completed!\n"]}],"source":["# ===================================================================================\n","\n","print(\"Training and Evaluation completed!\")\n","\n","# ===============================================================================\n","# Save output data to .excel file (we use for comparision plots)\n","round_process = [i for i in range(1, len(acc_train_collect) + 1)]\n","df = DataFrame({'round': round_process, 'acc_train': acc_train_collect, 'acc_test': acc_test_collect,\n","                'loss_train': loss_train_collect, 'loss_test': loss_test_collect})\n","file_name = program + \"_poisoned0.3_\" + dataset_choice + \".xlsx\"\n","df.to_excel(file_name, sheet_name=\"v1_test\", index=False)\n","\n","# =============================================================================\n","#                         Program Completed\n","# ============================================================================="]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}

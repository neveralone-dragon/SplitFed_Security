{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 一、预处理\n","\n","导入相关函数"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-09T09:22:15.939944Z","iopub.status.busy":"2023-04-09T09:22:15.939011Z","iopub.status.idle":"2023-04-09T09:22:15.950505Z","shell.execute_reply":"2023-04-09T09:22:15.949239Z","shell.execute_reply.started":"2023-04-09T09:22:15.939900Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NVIDIA GeForce GTX 1660 Ti\n"]}],"source":["import time\n","\n","# We have three versions of our implementations\n","# Version1: without using socket and no DP+PixelDP\n","# Version2: with using socket but no DP+PixelDP\n","# Version3: without using socket but with DP+PixelDP\n","\n","# This program is Version1: Single program simulation\n","# ============================================================================\n","import torch\n","from torch import nn\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import math\n","import os.path\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from glob import glob\n","from pandas import DataFrame\n","from collections import OrderedDict\n","\n","\n","import torchvision.datasets as datasets\n","import random\n","import numpy as np\n","import os\n","from torchvision.datasets import ImageFolder\n","\n","import matplotlib\n","\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_colwidth', 1000)\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import copy\n","\n","SEED = 1234\n","\n","def init_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.deterministic = True\n","        print(torch.cuda.get_device_name(0))\n","init_seeds(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.1模型定义"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.157118Z","iopub.status.busy":"2023-04-09T09:22:16.155896Z","iopub.status.idle":"2023-04-09T09:22:16.174878Z","shell.execute_reply":"2023-04-09T09:22:16.173785Z","shell.execute_reply.started":"2023-04-09T09:22:16.157073Z"},"trusted":true},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","class ResNet18_client_side(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(ResNet18_client_side, self).__init__()\n","        self.in_planes = 64\n","        self.conv1 = conv3x3(3, 64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        return out\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet18_server_side(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10, pool_size=4):  # Add a new argument pool_size\n","        super(ResNet18_server_side, self).__init__()\n","        self.in_planes = 64\n","        self.pool_size = pool_size  # Add this line to store pool_size\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.layer2(x)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        # print(\"Output shape before pooling:\", out.shape)\n","        out = F.avg_pool2d(out, kernel_size=self.pool_size)  # Use self.pool_size instead of 8\n","        # print(\"Output shape after pooling:\", out.shape)\n","        out = out.view(out.size(0), -1)\n","        y_hat = self.linear(out)\n","        return y_hat"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 模型初始化"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.374387Z","iopub.status.busy":"2023-04-09T09:22:16.373922Z","iopub.status.idle":"2023-04-09T09:22:16.390977Z","shell.execute_reply":"2023-04-09T09:22:16.389706Z","shell.execute_reply.started":"2023-04-09T09:22:16.374329Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------SFLV2_ResNet18_base----------\n","ResNet18_client_side(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n",")\n"]}],"source":["import logging\n","\n","def init_logging(program_name):\n","    logging.basicConfig(filename=f'{program_name}.log', level=logging.INFO)\n","    logging.getLogger().addHandler(logging.StreamHandler())\n","    \n","# ===================================================================\n","program = \"SFLV2_ResNet18_base\"\n","print(f\"---------{program}----------\")  # this is to identify the program in the slurm outputs files\n","init_logging(program)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# ===================================================================\n","# No. of users\n","num_users = 20\n","epochs = 100\n","frac = 1  # participation of clients; if 1 then 100% clients participate in SFLV2\n","lr = 0.0001\n","\n","# net_glob_client = ResNet18_client_side()\n","net_glob_client = ResNet18_client_side(BasicBlock, [2, 2, 2, 2]).to(device)\n","if torch.cuda.device_count() > 1:\n","    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n","    net_glob_client = nn.DataParallel(\n","        net_glob_client)  # to use the multiple GPUs; later we can change this to CPUs only\n","\n","net_glob_client.to(device)\n","print(net_glob_client)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Server Side Model 服务器端模型"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.394388Z","iopub.status.busy":"2023-04-09T09:22:16.393453Z","iopub.status.idle":"2023-04-09T09:22:16.522326Z","shell.execute_reply":"2023-04-09T09:22:16.520914Z","shell.execute_reply.started":"2023-04-09T09:22:16.394339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ResNet18_server_side(\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}],"source":["# =====================================================================================================\n","#                           Server-side Model definition\n","# =====================================================================================================\n","# Model at server side\n","# Choose dataset: 'HAM10000' or 'CIFAR10'\n","dataset_choice = 'CIFAR10'\n","# Model at server side\n","if dataset_choice == 'HAM10000':\n","    num_classes = 7\n","    pool_size = 8\n","elif dataset_choice == 'CIFAR10':\n","    num_classes = 10\n","    pool_size = 4\n","else:\n","    raise ValueError('Invalid dataset choice.')\n","\n","net_glob_server = ResNet18_server_side(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, pool_size=pool_size)\n","# net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 7) #7 is my numbr of classes\n","if torch.cuda.device_count() > 1:\n","    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n","    net_glob_server = nn.DataParallel(net_glob_server)  # to use the multiple GPUs\n","\n","net_glob_server.to(device)\n","print(net_glob_server)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 变量初始化"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:16.598584Z","iopub.status.busy":"2023-04-09T09:22:16.598245Z","iopub.status.idle":"2023-04-09T09:22:16.606087Z","shell.execute_reply":"2023-04-09T09:22:16.604922Z","shell.execute_reply.started":"2023-04-09T09:22:16.598552Z"},"trusted":true},"outputs":[],"source":["# ===================================================================================\n","# For Server Side Loss and Accuracy\n","loss_train_collect = []\n","acc_train_collect = []\n","loss_test_collect = []\n","acc_test_collect = []\n","batch_acc_train = []\n","batch_loss_train = []\n","batch_acc_test = []\n","batch_loss_test = []\n","\n","criterion = nn.CrossEntropyLoss()\n","count1 = 0\n","count2 = 0\n","\n","# ====================================================================================================\n","#                                  Server Side Programs\n","# ====================================================================================================\n","# Federated averaging: FedAvg\n","# to print train - test together in each round-- these are made global\n","acc_avg_all_user_train = 0\n","loss_avg_all_user_train = 0\n","loss_train_collect_user = []\n","acc_train_collect_user = []\n","loss_test_collect_user = []\n","acc_test_collect_user = []\n","\n","# client idx collector\n","idx_collect = []\n","l_epoch_check = False\n","fed_check = False"]},{"cell_type":"markdown","metadata":{},"source":["## 1.5 Server端训练定义"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.040140Z","iopub.status.busy":"2023-04-09T09:22:17.039174Z","iopub.status.idle":"2023-04-09T09:22:17.062363Z","shell.execute_reply":"2023-04-09T09:22:17.061299Z","shell.execute_reply.started":"2023-04-09T09:22:17.040091Z"},"trusted":true},"outputs":[],"source":["# Server-side function associated with Training\n","def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n","    \"\"\"\n","\n","    :param fx_client:客户端计算得到的特征值\n","    :param y:对应的标签\n","    :param l_epoch_count:本地训练周期计数\n","    :param l_epoch:本地训练周期总数\n","    :param idx:\n","    :param len_batch:\n","    :return:\n","    \"\"\"\n","    global net_glob_server, criterion, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n","    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect\n","    global loss_train_collect_user, acc_train_collect_user, lr\n","\n","    net_glob_server.train()\n","    optimizer_server = torch.optim.Adam(net_glob_server.parameters(), lr=lr)\n","\n","    # train and update\n","    optimizer_server.zero_grad()\n","\n","    fx_client = fx_client.to(device)\n","    y = y.to(device)\n","\n","    # ---------forward prop-------------\n","    # 将fx_client输入到服务器端模型net_glob_server，计算得到预测值fx_server\n","    fx_server = net_glob_server(fx_client)\n","\n","    # calculate loss\n","    loss = criterion(fx_server, y)\n","    # calculate accuracy\n","    acc = calculate_accuracy(fx_server, y)\n","\n","    # --------backward prop--------------\n","    loss.backward()\n","    # 将fx_client的梯度保存为dfx_client。\n","    dfx_client = fx_client.grad.clone().detach()\n","    optimizer_server.step()\n","\n","    batch_loss_train.append(loss.item())\n","    batch_acc_train.append(acc.item())\n","\n","    # server-side model net_glob_server is global so it is updated automatically in each pass to this function\n","\n","    # count1: to track the completion of the local batch associated with one client\n","    count1 += 1  # 增加计数器count1，用于追踪与一个客户端关联的本地批次的完成情况。\n","    if count1 == len_batch:\n","        # 如果count1等于len_batch，则计算批次的平均损失和准确度，并将它们分别存储在acc_avg_train和loss_avg_train中。然后清空\n","        acc_avg_train = sum(batch_acc_train) / len(batch_acc_train)  # it has accuracy for one batch\n","        loss_avg_train = sum(batch_loss_train) / len(batch_loss_train)\n","\n","        batch_acc_train = []\n","        batch_loss_train = []\n","        count1 = 0\n","\n","        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train,\n","                                                                                      loss_avg_train))\n","\n","        # If one local epoch is completed, after this a new client will come\n","        if l_epoch_count == l_epoch - 1:\n","\n","            l_epoch_check = True  # to evaluate_server function - to check local epoch has completed or not\n","\n","            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n","            # this is because we work on the last trained model and its accuracy (not earlier cases)\n","\n","            # print(\"accuracy = \", acc_avg_train)\n","            acc_avg_train_all = acc_avg_train\n","            loss_avg_train_all = loss_avg_train\n","\n","            # accumulate accuracy and loss for each new user\n","            loss_train_collect_user.append(loss_avg_train_all)\n","            acc_train_collect_user.append(acc_avg_train_all)\n","\n","            # collect the id of each new user\n","            if idx not in idx_collect:\n","                idx_collect.append(idx)\n","                # print(idx_collect)\n","\n","        # This is to check if all users are served for one round --------------------\n","        if len(idx_collect) == num_users * frac:\n","            fed_check = True  # to evaluate_server function  - to check fed check has hitted\n","            # all users served for one round ------------------------- output print and update is done in evaluate_server()\n","            # for nicer display\n","\n","            idx_collect = []\n","\n","            acc_avg_all_user_train = sum(acc_train_collect_user) / len(acc_train_collect_user)\n","            loss_avg_all_user_train = sum(loss_train_collect_user) / len(loss_train_collect_user)\n","\n","            loss_train_collect.append(loss_avg_all_user_train)\n","            acc_train_collect.append(acc_avg_all_user_train)\n","\n","            acc_train_collect_user = []\n","            loss_train_collect_user = []\n","\n","    # send gradients to the client\n","    return dfx_client\n","\n","\n","# Server-side functions associated with Testing\n","def evaluate_server(fx_client, y, idx, len_batch, ell, selected_clients):\n","    global net_glob_server, criterion, batch_acc_test, batch_loss_test\n","    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, l_epoch_check, fed_check\n","    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n","\n","    net_glob_server.eval()\n","    return_local_results = False\n","\n","    with torch.no_grad():\n","        fx_client = fx_client.to(device)\n","        y = y.to(device)\n","        # ---------forward prop-------------\n","        fx_server = net_glob_server(fx_client)\n","\n","        # calculate loss\n","        loss = criterion(fx_server, y)\n","        # calculate accuracy\n","        acc = calculate_accuracy(fx_server, y)\n","\n","        batch_loss_test.append(loss.item())\n","        batch_acc_test.append(acc.item())\n","\n","        # 计数器 count2 用于跟踪处理的客户端数量。如果已经处理了一个批次的客户端数据，那么计算批次的平均损失和平均准确度，然后重置批次损失和批次准确度列表以及计数器。\n","        count2 += 1\n","        if count2 == len_batch:\n","            acc_avg_test = sum(batch_acc_test) / len(batch_acc_test)\n","            loss_avg_test = sum(batch_loss_test) / len(batch_loss_test)\n","\n","            batch_acc_test = []\n","            batch_loss_test = []\n","            count2 = 0\n","\n","            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test,\n","                                                                                             loss_avg_test))\n","\n","            # if a local epoch is completed\n","            if l_epoch_check:\n","                l_epoch_check = False\n","                return_local_results = True\n","\n","                # Store the last accuracy and loss\n","                acc_avg_test_all = acc_avg_test\n","                loss_avg_test_all = loss_avg_test\n","\n","                loss_test_collect_user.append(loss_avg_test_all)\n","                acc_test_collect_user.append(acc_avg_test_all)\n","\n","            # if all users are served for one round ----------\n","            if fed_check:\n","                fed_check = False\n","\n","\n","\n","                # 计算Krum选定客户端的平均准确率和损失\n","                if selected_clients is None or len(selected_clients) == 0:\n","                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n","                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n","                else:\n","                    print(\"选择的客户端index:\",selected_clients)\n","                    acc_test_collect_user = [acc_test_collect_user[i] for i in selected_clients]\n","                    loss_test_collect_user = [loss_test_collect_user[i] for i in selected_clients]\n","\n","                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n","                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n","\n","\n","                loss_test_collect.append(loss_avg_all_user)\n","                acc_test_collect.append(acc_avg_all_user)\n","                acc_test_collect_user = []\n","                loss_test_collect_user = []\n","\n","                print(\"====================== SERVER V1==========================\")\n","                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train,\n","                                                                                          loss_avg_all_user_train))\n","                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user,\n","                                                                                         loss_avg_all_user))\n","                print(\"==========================================================\")\n","\n","    if return_local_results:\n","        return acc_avg_test_all,loss_avg_test_all\n","    else:\n","        return None,None"]},{"cell_type":"markdown","metadata":{},"source":["## 1.6 Client端类定义"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.176954Z","iopub.status.busy":"2023-04-09T09:22:17.176609Z","iopub.status.idle":"2023-04-09T09:22:17.190060Z","shell.execute_reply":"2023-04-09T09:22:17.188999Z","shell.execute_reply.started":"2023-04-09T09:22:17.176923Z"},"trusted":true},"outputs":[],"source":["class Client(object):\n","    def __init__(self, net_client_model, idx, lr, device, dataset_train=None, dataset_test=None, idxs=None,\n","                 idxs_test=None, is_attacker=None, batch_size=128):\n","        \"\"\"\n","        :param idxs: idxs是一个表示该客户端用于训练的数据集的索引列表。在联邦学习中，原始数据集通常由多个客户端持有，每个客户端只能访问自己所持有的部分数据集。因此，为了让每个客户端只使用自己所持有的数据进行训练，需要将原始数据集划分成多个部分，每个部分由一个客户端持有，并通过idxs将该客户端用于训练的数据集的索引列表传递给Client类的构造函数。\n","        :param idxs_test:\n","        \"\"\"\n","        # net_client_model:一个与客户端实例相关的神经网络模型。\n","        self.batch_size = batch_size\n","        self.is_attacker = is_attacker\n","        self.idx = idx  # 一个整数，表示客户端的索引\n","        self.device = device  # 一个字符串，表示执行客户端计算的设备。\n","        self.lr = lr\n","        self.local_ep = 1\n","        # self.selected_clients = []\n","        # DatasetSplit(dataset_train, idxs)表示使用DatasetSplit类将原始的数据集dataset_train按照索引idxs进行划分，以获得当前客户端可用于训练的数据集。\n","        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size=self.batch_size,\n","                                    shuffle=True)\n","        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self, net):\n","        net.train()\n","        optimizer_client = torch.optim.Adam(net.parameters(), lr=self.lr)\n","\n","        for iter in range(self.local_ep):\n","            if self.is_attacker:\n","                dataset_split = self.ldr_train.dataset\n","                num_malicious_samples = len(dataset_split.idxs)\n","                dataset_split.add_malicious_samples(num_malicious_samples)\n","\n","                # Refresh the DataLoader after adding malicious samples\n","                self.ldr_train = DataLoader(dataset_split, batch_size=self.batch_size, shuffle=True)\n","\n","            # 外层循环是客户端的本地训练轮数self.local_ep\n","            len_batch = len(self.ldr_train)  # 计算该客户端的训练集数据分成的批次数。\n","            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n","                # 内层循环是数据加载器self.ldr_train中每个批次的训练。在每个批次中，将图像和标签加载到设备上，然后将优化器的梯度清零。\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                optimizer_client.zero_grad()\n","                # ---------forward prop-------------\n","                fx = net(images)\n","                # 生成一个可求导的副本client_fx\n","                client_fx = fx.clone().detach().requires_grad_(True)\n","\n","                # Sending activations to server and receiving gradients from server\n","                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n","\n","                # --------backward prop -------------\n","                fx.backward(dfx)\n","                optimizer_client.step()\n","\n","            # prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n","\n","        return net.state_dict()\n","\n","    def evaluate(self, net=None, ell=None, selected_clients=None):\n","        net.eval()\n","\n","        with torch.no_grad():\n","            len_batch = len(self.ldr_test)\n","            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                # ---------forward prop-------------\n","                fx = net(images)\n","\n","                # Sending activations to server\n","                acc_avg_test_all, loss_avg_test_all = evaluate_server(fx, labels, self.idx, len_batch, ell,\n","                                                                      selected_clients)\n","\n","            # prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n","            if loss_avg_test_all is not None and acc_avg_test_all is not None:\n","                self.loss_avg_test_all = loss_avg_test_all\n","                self.acc_avg_test_all = acc_avg_test_all\n","\n","        return"]},{"cell_type":"markdown","metadata":{},"source":["## 1.7 聚合算法定义"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.255618Z","iopub.status.busy":"2023-04-09T09:22:17.254634Z","iopub.status.idle":"2023-04-09T09:22:17.263408Z","shell.execute_reply":"2023-04-09T09:22:17.262128Z","shell.execute_reply.started":"2023-04-09T09:22:17.255579Z"},"trusted":true},"outputs":[],"source":["# To print in color -------test/train of the client side\n","def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n","def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n","\n","def calculate_accuracy(fx, y):\n","    preds = fx.max(1, keepdim=True)[1]\n","    correct = preds.eq(y.view_as(preds)).sum()\n","    acc = 100.00 *correct.float()/preds.shape[0]\n","    return acc\n","# Federated averaging: FedAvg\n","def FedAvg(w):\n","    w_avg = copy.deepcopy(w[0])\n","    for k in w_avg.keys():\n","        for i in range(1, len(w)):\n","            w_avg[k] += w[i][k]\n","        w_avg[k] = torch.div(w_avg[k], len(w))\n","    return w_avg\n","\n","def krum_aggregation(weight_dicts, num_to_select):\n","    # ...（weights_to_array、array_to_weights和pairwise_distances函数保持不变）\n","    def weights_to_array(weight_dict):\n","        \"\"\"\n","        将权重字典转换为 numpy 数组。\n","        :param weight_dict: 权重字典，其中每个值都是一个 torch.Tensor 张量。\n","        :return: 一个 numpy 数组，其中包含了所有权重张量的扁平化数组。\n","        \"\"\"\n","        # 初始化 weight_list 列表，用于存储 weight_dict 中每个权重张量的扁平化数组。\n","        weight_list = []\n","        for key in weight_dict:\n","            # 将权重张量转换为 numpy 数组，并使用 flatten 函数将其扁平化\n","            weight_list.append(weight_dict[key].cpu().numpy().flatten())\n","        # 将 weight_list 中的数组连接成一个 numpy 数组，并返回该数组\n","        return np.concatenate(weight_list)\n","\n","    def array_to_weights(array, weight_dict_template):\n","        \"\"\"\n","        从 numpy 数组中提取权重张量，并将它们保存到一个新的权重字典中。\n","        :param array: 包含所有权重张量的扁平化 numpy 数组。\n","        :param weight_dict_template: 一个权重字典模板，其中包含了所有权重张量的形状。\n","        :return: 一个新的权重字典，其中包含了从数组中提取的权重张量。\n","        \"\"\"\n","        # 初始化一个新的有序字典 new_weight_dict，用于存储从 numpy 数组中提取的权重张量\n","        new_weight_dict = OrderedDict()\n","        # 初始化一个索引变量 idx，用于跟踪从数组中提取权重的位置\n","        idx = 0\n","        # 遍历权重字典模板中的每个键 key\n","        for key in weight_dict_template:\n","            # 获取权重张量的大小 size\n","            size = weight_dict_template[key].numel()\n","            # 从 numpy 数组中提取一个与权重张量相同大小的一维切片，并使用 reshape 函数重新塑形\n","            # 将 numpy 数组转换为 torch.Tensor 张量，并将其存储到新的有序字典 new_weight_dict 中\n","            new_weight_dict[key] = torch.from_numpy(array[idx:idx + size].reshape(weight_dict_template[key].shape))\n","            # 更新索引变量 idx 的值，跳过已经提取的权重张量\n","            idx += size\n","        # 返回新的有序字典 new_weight_dict\n","        return new_weight_dict\n","\n","    def pairwise_distances(weight_updates):\n","        # 获取客户端数量\n","        n_clients = len(weight_updates)\n","        # 初始化一个距离矩阵，矩阵大小为 n_clients * n_clients\n","        distances = np.zeros((n_clients, n_clients))\n","\n","        # 遍历所有的客户端对\n","        for i in range(n_clients):\n","            for j in range(i + 1, n_clients):\n","                # 计算 i 和 j 客户端之间的欧氏距离\n","                dist = np.linalg.norm(weight_updates[i] - weight_updates[j])\n","                # 在距离矩阵中记录距离\n","                distances[i, j] = dist\n","                distances[j, i] = dist\n","\n","        # 返回距离矩阵\n","        return distances\n","\n","    weight_arrays = [weights_to_array(weight_dict) for weight_dict in weight_dicts]\n","    n_clients = len(weight_arrays)\n","    distances = pairwise_distances(weight_arrays)\n","\n","    krum_scores = []\n","    for i in range(n_clients):\n","        sorted_distances = np.sort(distances[i])\n","        # 计算当前客户端的Krum分数，即距离最大的n_clients - num_to_select - 1个客户端之间的距离总和。\n","        krum_score = np.sum(sorted_distances[-(n_clients - num_to_select - 1):])\n","        krum_scores.append(krum_score)\n","\n","    # 使用np.argpartition函数找到具有最低Krum分数的num_to_select个客户端的索引。\n","    best_clients_indices = np.argpartition(krum_scores, num_to_select)[:num_to_select]\n","\n","    # 从权重数组中选择最佳客户端的权重。\n","    selected_weight_arrays = [weight_arrays[i] for i in best_clients_indices]\n","    # 计算选定客户端的权重数组的平均值，得到聚合后的权重数组。\n","    aggregated_weight_array = np.mean(selected_weight_arrays, axis=0)\n","\n","    # 使用array_to_weights函数将聚合后的权重数组转换回权重字典。\n","    aggregated_weights = array_to_weights(aggregated_weight_array, weight_dicts[0])\n","\n","    # 返回聚合后的权重字典和最好的客户端索引列表。\n","    return aggregated_weights, best_clients_indices"]},{"cell_type":"markdown","metadata":{},"source":["# 2.Data Loading"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 Data处理函数定义"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.672259Z","iopub.status.busy":"2023-04-09T09:22:17.671890Z","iopub.status.idle":"2023-04-09T09:22:17.701920Z","shell.execute_reply":"2023-04-09T09:22:17.700534Z","shell.execute_reply.started":"2023-04-09T09:22:17.672224Z"},"trusted":true},"outputs":[],"source":["from glob import glob\n","from sklearn.model_selection import train_test_split\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image\n","from PIL import ImageEnhance\n","\n","class DatasetSplit(Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = list(idxs)\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, label\n","\n","    def add_malicious_samples(self, num_malicious_samples):\n","        for i in range(num_malicious_samples):\n","            # Randomly select an image to modify\n","            dataset_idx = random.choice(self.idxs)\n","            img = self.dataset.data[dataset_idx]\n","\n","            # Check if the image has the shape (3, 32, 32)\n","            if img.shape == (3, 32, 32):\n","                img = np.transpose(img, (1, 2, 0))  # Change the shape to (32, 32, 3)\n","\n","            # Convert the numpy array to a PIL Image\n","            img = Image.fromarray(img)\n","\n","            # Apply a random modification to the image (e.g., flip, rotate, or change color)\n","            # In this example, we flip the image horizontally, rotate, and change the color\n","            modified_img = img.transpose(Image.FLIP_LEFT_RIGHT)  # Flip the image horizontally\n","            modified_img = modified_img.rotate(random.randint(0, 360))  # Rotate the image randomly\n","\n","            # Change the color by adjusting the brightness, contrast, and saturation\n","            brightness = ImageEnhance.Brightness(modified_img)\n","            modified_img = brightness.enhance(random.uniform(0.5, 1.5))\n","\n","            contrast = ImageEnhance.Contrast(modified_img)\n","            modified_img = contrast.enhance(random.uniform(0.5, 1.5))\n","\n","            saturation = ImageEnhance.Color(modified_img)\n","            modified_img = saturation.enhance(random.uniform(0.5, 1.5))\n","\n","            # Convert the modified PIL Image back to a numpy array\n","            modified_img = np.array(modified_img)\n","\n","            # Check if the original image has the shape (3, 32, 32)\n","            if self.dataset.data[dataset_idx].shape == (3, 32, 32):\n","                modified_img = np.transpose(modified_img, (2, 0, 1))  # Change the shape back to (3, 32, 32)\n","\n","            # Update the image in the dataset\n","            self.dataset.data[dataset_idx] = modified_img\n","\n","            # # Change the label of the modified image to a random class\n","            # random_label = random.randint(0, 9)  # Assuming 10 classes in the dataset\n","            # self.dataset.targets[dataset_idx] = random_label\n","\n","class SkinData(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        X = Image.open(self.df['path'][index]).resize((64, 64))\n","        y = torch.tensor(int(self.df['target'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y\n","\n","def dataset_iid(dataset, num_users):\n","    \"\"\"\n","    该函数接受一个数据集dataset和一个整数num_users作为输入。\n","    它的作用是将数据集分割成num_users份，以便每个客户端都有一份相同分布的数据集。\n","    :param dataset:\n","    :param num_users:\n","    :return:函数返回一个字典dict_users，其中包含num_users个键，每个键对应一个客户端，值为该客户端所分配的数据集索引的集合。\n","    dict_users:{idx:int : []:list}\n","    \"\"\"\n","    # 该函数首先计算每个客户端应该拥有的数据量num_items\n","    num_items = int(len(dataset)/num_users)\n","    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n","    for i in range(num_users):\n","        # 接着，函数使用np.random.choice函数从all_idxs中选择num_items个索引，将这些索引添加到字典dict_users的第i个键中，表示第i个客户端的数据集。在选择后，从all_idxs中移除已经分配给第i个客户端的索引。\n","        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n","        all_idxs = list(set(all_idxs) - dict_users[i])\n","    return dict_users\n","\n","def load_data(dataset_choice, num_users):\n","    \"\"\"\n","\n","    :param dataset_choice: 选择的数据集\n","    :param num_users: 用户的数量\n","    :return:\n","    \"\"\"\n","    if dataset_choice == 'HAM10000':\n","        df = pd.read_csv('data/HAM10000_metadata.csv')\n","        print(df.head())\n","\n","        lesion_type = {\n","            'nv': 'Melanocytic nevi',\n","            'mel': 'Melanoma',\n","            'bkl': 'Benign keratosis-like lesions ',\n","            'bcc': 'Basal cell carcinoma',\n","            'akiec': 'Actinic keratoses',\n","            'vasc': 'Vascular lesions',\n","            'df': 'Dermatofibroma'\n","        }\n","\n","        # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n","        imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n","                        for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n","\n","        # print(\"path---------------------------------------\", imageid_path.get)\n","        # 将图像id映射为图像文件的路径，并将其存储在数据集中的path列中。\n","        df['path'] = df['image_id'].map(imageid_path.get)\n","        # 将诊断编码映射为对应的分类名称，并将其存储在数据集中的cell_type列中。\n","        df['cell_type'] = df['dx'].map(lesion_type.get)\n","        # 将分类名称转换为数字编码，并将其存储在数据集中的target列中。这里使用了.\n","        # 可以将字符串类型的分类变量转换为数字编码，其中不同的分类名称对应不同的数字编码。\n","        df['target'] = pd.Categorical(df['cell_type']).codes\n","        print(df['cell_type'].value_counts())\n","        print(df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train, test = train_test_split(df, test_size=0.2)\n","\n","        train = train.reset_index()\n","\n","        test = test.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(64),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(64),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = SkinData(train, transform=train_transforms)\n","        dataset_test = SkinData(test, transform=test_transforms)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    elif dataset_choice == 'CIFAR10':\n","        # =============================================================================\n","        #                         Data loading\n","        # =============================================================================\n","        # Load CIFAR-10 dataset\n","        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n","        testset = datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","        train_df = pd.DataFrame(trainset.targets, columns=['target'])\n","        test_df = pd.DataFrame(testset.targets, columns=['target'])\n","\n","        # Set the class names\n","        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","        train_df['cell_type'] = train_df['target'].apply(lambda x: class_names[x])\n","        test_df['cell_type'] = test_df['target'].apply(lambda x: class_names[x])\n","\n","        print(train_df['cell_type'].value_counts())\n","        print(train_df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train = train_df.reset_index()\n","        test = test_df.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(32),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(32),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n","        dataset_test = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    else:\n","        raise ValueError(\"Invalid dataset_choice: Choose either 'HAM10000' or 'CIFAR10'\")\n","\n","    return dataset_train,dataset_test,dict_users,dict_users_test"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 load data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:17.704519Z","iopub.status.busy":"2023-04-09T09:22:17.704050Z","iopub.status.idle":"2023-04-09T09:22:21.052202Z","shell.execute_reply":"2023-04-09T09:22:21.051134Z","shell.execute_reply.started":"2023-04-09T09:22:17.704480Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","frog          5000\n","truck         5000\n","deer          5000\n","automobile    5000\n","bird          5000\n","horse         5000\n","ship          5000\n","cat           5000\n","dog           5000\n","airplane      5000\n","Name: cell_type, dtype: int64\n","6    5000\n","9    5000\n","4    5000\n","1    5000\n","2    5000\n","7    5000\n","8    5000\n","3    5000\n","5    5000\n","0    5000\n","Name: target, dtype: int64\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# =============================================================================\n","#                         Data loading\n","# =============================================================================\n","dataset_train, dataset_test, dict_users, dict_users_test = load_data(dataset_choice, num_users)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2.3数据投毒——标签翻转\n","### 2.3.1函数定义"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:21.054947Z","iopub.status.busy":"2023-04-09T09:22:21.054270Z","iopub.status.idle":"2023-04-09T09:22:21.067110Z","shell.execute_reply":"2023-04-09T09:22:21.066112Z","shell.execute_reply.started":"2023-04-09T09:22:21.054905Z"},"trusted":true},"outputs":[],"source":["def replace_label1_with_label2_on_df(df,label1,label2,poisoned_dict_users):\n","    \"\"\"\n","    标签反转\n","    :param df:dataframe\n","    :param label1: 等待翻转的标签\n","    :param label2: 需要翻转的标签\n","    :param poisoned_dict_users:包含索引列表的字典,中毒的用户\n","    :return:\n","    \"\"\"\n","    for idx_list in poisoned_dict_users.values():\n","        for idx in idx_list:\n","            if df.loc[idx,'target'] == label1:\n","                df.loc[idx,'target'] = label2\n","    # df.loc[df['target'] == label1, 'target'] = label2\n","    return df\n","\n","def random_select_poisoning_users(dict_users, n):\n","    \"\"\"\n","    随机选择n个key-value对\n","    :param dict_users: 字典，key为索引，value为包含索引值的列表\n","    :param n: 选择的key-value对的数量\n","    :return: 随机选择的key-value对组成的字典\n","    \"\"\"\n","    selected = {}\n","    keys = random.sample(list(dict_users.keys()), n)\n","    for k in keys:\n","        selected[k] = dict_users[k]\n","    return selected\n","\n","# def poison_data(dataset_train, dict_users, poisoned_users_num, original_label, target_label):\n","#     poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n","#     replace_label1_with_label2_on_df(dataset_train.df, original_label, target_label, poisoned_dict_users)\n","#     return poisoned_dict_users\n","\n","def print_poisoning_results(poisoned_dict_users, dataset_train):\n","    for poisoned_user_key in poisoned_dict_users:\n","        print(\"被投毒的用户:\", poisoned_user_key)\n","\n","    print(\"标签反转后的target统计:\")\n","    print(dataset_train.df['target'].value_counts())\n","\n","def cifar10_to_dataframe(dataset):\n","    data = [dataset[i] for i in range(len(dataset))]\n","    images, labels = zip(*data)\n","    df = pd.DataFrame({\"image\": images, \"target\": labels})\n","    return df\n","\n","\n","\n","def poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings):\n","    if dataset_choice == 'CIFAR10':\n","        df_train = cifar10_to_dataframe(dataset_train)\n","    elif dataset_choice == 'HAM10000':\n","        df_train = dataset_train.df\n","    else:\n","        raise ValueError(\"Invalid dataset choice.\")\n","\n","    poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n","\n","    for original_label, target_label in label_mappings:\n","        replace_label1_with_label2_on_df(df_train, original_label, target_label, poisoned_dict_users)\n","\n","    # 修改 dataset_train 的标签\n","    if dataset_choice == 'CIFAR10':\n","        for idx, row in df_train.iterrows():\n","            dataset_train.targets[idx] = row['target']\n","\n","    return poisoned_dict_users\n","\n","def load_data(dataset_choice, num_users):\n","    \"\"\"\n","\n","    :param dataset_choice: 选择的数据集\n","    :param num_users: 用户的数量\n","    :return:\n","    \"\"\"\n","    if dataset_choice == 'HAM10000':\n","        df = pd.read_csv('data/HAM10000_metadata.csv')\n","        print(df.head())\n","\n","        lesion_type = {\n","            'nv': 'Melanocytic nevi',\n","            'mel': 'Melanoma',\n","            'bkl': 'Benign keratosis-like lesions ',\n","            'bcc': 'Basal cell carcinoma',\n","            'akiec': 'Actinic keratoses',\n","            'vasc': 'Vascular lesions',\n","            'df': 'Dermatofibroma'\n","        }\n","\n","        # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n","        imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n","                        for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n","\n","        # print(\"path---------------------------------------\", imageid_path.get)\n","        # 将图像id映射为图像文件的路径，并将其存储在数据集中的path列中。\n","        df['path'] = df['image_id'].map(imageid_path.get)\n","        # 将诊断编码映射为对应的分类名称，并将其存储在数据集中的cell_type列中。\n","        df['cell_type'] = df['dx'].map(lesion_type.get)\n","        # 将分类名称转换为数字编码，并将其存储在数据集中的target列中。这里使用了.\n","        # 可以将字符串类型的分类变量转换为数字编码，其中不同的分类名称对应不同的数字编码。\n","        df['target'] = pd.Categorical(df['cell_type']).codes\n","        print(df['cell_type'].value_counts())\n","        print(df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train, test = train_test_split(df, test_size=0.2)\n","\n","        train = train.reset_index()\n","\n","        test = test.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(64),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(64),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = SkinData(train, transform=train_transforms)\n","        dataset_test = SkinData(test, transform=test_transforms)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    elif dataset_choice == 'CIFAR10':\n","        # =============================================================================\n","        #                         Data loading\n","        # =============================================================================\n","        # Load CIFAR-10 dataset\n","        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n","        testset = datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","        train_df = pd.DataFrame(trainset.targets, columns=['target'])\n","        test_df = pd.DataFrame(testset.targets, columns=['target'])\n","\n","        # Set the class names\n","        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","        train_df['cell_type'] = train_df['target'].apply(lambda x: class_names[x])\n","        test_df['cell_type'] = test_df['target'].apply(lambda x: class_names[x])\n","\n","        print(train_df['cell_type'].value_counts())\n","        print(train_df['target'].value_counts())\n","\n","        # =============================================================================\n","        # Train-test split\n","        train = train_df.reset_index()\n","        test = test_df.reset_index()\n","\n","        # =============================================================================\n","        #                         Data preprocessing\n","        # =============================================================================\n","        # Data preprocessing: Transformation\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                               transforms.RandomVerticalFlip(),\n","                                               transforms.Pad(3),\n","                                               transforms.RandomRotation(10),\n","                                               transforms.CenterCrop(32),\n","                                               transforms.ToTensor(),\n","                                               transforms.Normalize(mean=mean, std=std)\n","                                               ])\n","\n","        test_transforms = transforms.Compose([\n","            transforms.Pad(3),\n","            transforms.CenterCrop(32),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)\n","        ])\n","\n","        # With augmentation\n","        dataset_train = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n","        dataset_test = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n","\n","        # ----------------------------------------------------------------\n","        dict_users = dataset_iid(dataset_train, num_users)\n","        dict_users_test = dataset_iid(dataset_test, num_users)\n","    else:\n","        raise ValueError(\"Invalid dataset_choice: Choose either 'HAM10000' or 'CIFAR10'\")\n","\n","    return dataset_train,dataset_test,dict_users,dict_users_test"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:22:21.070723Z","iopub.status.busy":"2023-04-09T09:22:21.069778Z","iopub.status.idle":"2023-04-09T09:23:27.457519Z","shell.execute_reply":"2023-04-09T09:23:27.456364Z","shell.execute_reply.started":"2023-04-09T09:22:21.070694Z"},"trusted":true},"outputs":[],"source":["def generate_label_mappings(dataset_choice):\n","    if dataset_choice == 'CIFAR10':\n","        num_classes = 10\n","    elif dataset_choice == 'HAM10000':\n","        num_classes = 7\n","    else:\n","        raise ValueError(\"Invalid dataset choice.\")\n","\n","    label_mappings = []\n","    for i in range(num_classes):\n","        target_label = random.choice([j for j in range(num_classes) if j != i])\n","        label_mappings.append((i, target_label))\n","\n","    return label_mappings\n","\n","# =============================================================================\n","#                         Poisoning\n","# =============================================================================\n","# 增加投毒用户数量至 50%\n","# poisoned_frac = 0.5\n","# poisoned_users_num = int(poisoned_frac * num_users)\n","# # label_mappings = [(4, 2), (1, 7), (3, 1), (2, 6), (6, 5)]\n","# label_mappings = generate_label_mappings(dataset_choice)\n","# poisoned_dict_users = poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings)\n","# # poisoned_dict_users = poison_data_random(dataset_train, dataset_choice, dict_users, poisoned_users_num)\n","# # poisoned_dict_users = poison_data_model(dataset_train, dataset_choice, dict_users, poisoned_users_num, attack_pattern, target_label)\n","\n","\n","# for poisoned_user_key in poisoned_dict_users:\n","#     print(\"被投毒的用户:\", poisoned_user_key)\n","\n","# # 如果使用的是 HAM10000 数据集，您可以直接使用 dataset_train.df 查看标签分布\n","# if dataset_choice == 'HAM10000':\n","#     print(\"标签反转后的target统计:\")\n","#     print(dataset_train.df['target'].value_counts())\n","# elif dataset_choice == 'CIFAR10':\n","#     cifar10_df = cifar10_to_dataframe(dataset_train)\n","#     print(\"标签反转后的target统计:\")\n","#     print(cifar10_df['target'].value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["# 3.模型训练和评估"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:23:27.460485Z","iopub.status.busy":"2023-04-09T09:23:27.460129Z","iopub.status.idle":"2023-04-09T09:23:27.468395Z","shell.execute_reply":"2023-04-09T09:23:27.467192Z","shell.execute_reply.started":"2023-04-09T09:23:27.460448Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["匹配攻击选取的客户端 [11  4 14 18  8  1]\n"]}],"source":["# ------------ Training And Testing -----------------\n","net_glob_client.train()\n","# copy weights\n","w_glob_client = net_glob_client.state_dict()\n","\n","# Federation takes place after certain local epochs in train() client-side\n","# this epoch is global epoch, also known as rounds\n","\n","total_time = 0.0  # 初始化总时间为0\n","best_clients_indices = None\n","krum_acc_test_collect = []\n","krum_loss_test_collect = []\n","\n","# 梯度匹配攻击\n","# Choose a fraction of clients to be attackers\n","attackers_frac = 0.3  # Modify this value according to your requirements\n","num_attackers = int(attackers_frac * num_users)\n","attackers_indices = np.random.choice(num_users, num_attackers, replace=False)\n","print(\"匹配攻击选取的客户端\",str(attackers_indices))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T09:23:27.470769Z","iopub.status.busy":"2023-04-09T09:23:27.470390Z","iopub.status.idle":"2023-04-09T11:13:02.195066Z","shell.execute_reply":"2023-04-09T11:13:02.193866Z","shell.execute_reply.started":"2023-04-09T09:23:27.470731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 25.110 \tLoss: 2.0389\u001b[00m\n","\u001b[92m Client10 Test =>                   \tAcc: 8.015 \tLoss: 2.8789\u001b[00m\n","\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 30.165 \tLoss: 1.8810\u001b[00m\n","\u001b[92m Client5 Test =>                   \tAcc: 10.574 \tLoss: 2.8769\u001b[00m\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_1348\\176596015.py:34: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n","  modified_img = img.transpose(Image.FLIP_LEFT_RIGHT)  # Flip the image horizontally\n"]},{"ename":"NameError","evalue":"name 'ImageEnhance' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m local \u001b[39m=\u001b[39m Client(net_glob_client, idx, lr, device, dataset_train\u001b[39m=\u001b[39mdataset_train, dataset_test\u001b[39m=\u001b[39mdataset_test,\n\u001b[0;32m     13\u001b[0m                idxs\u001b[39m=\u001b[39mdict_users[idx], idxs_test\u001b[39m=\u001b[39mdict_users_test[idx], is_attacker\u001b[39m=\u001b[39mis_attacker)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Training ------------------\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m w_client \u001b[39m=\u001b[39m local\u001b[39m.\u001b[39;49mtrain(net\u001b[39m=\u001b[39;49mcopy\u001b[39m.\u001b[39;49mdeepcopy(net_glob_client)\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     16\u001b[0m idx_local_client_dict[\u001b[39mlen\u001b[39m(w_locals_client)] \u001b[39m=\u001b[39m idx\n\u001b[0;32m     17\u001b[0m w_locals_client\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39mdeepcopy(w_client))\n","Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mClient.train\u001b[1;34m(self, net)\u001b[0m\n\u001b[0;32m     27\u001b[0m dataset_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mldr_train\u001b[39m.\u001b[39mdataset\n\u001b[0;32m     28\u001b[0m num_malicious_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset_split\u001b[39m.\u001b[39midxs)\n\u001b[1;32m---> 29\u001b[0m dataset_split\u001b[39m.\u001b[39;49madd_malicious_samples(num_malicious_samples)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Refresh the DataLoader after adding malicious samples\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mldr_train \u001b[39m=\u001b[39m DataLoader(dataset_split, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","Cell \u001b[1;32mIn[9], line 38\u001b[0m, in \u001b[0;36mDatasetSplit.add_malicious_samples\u001b[1;34m(self, num_malicious_samples)\u001b[0m\n\u001b[0;32m     35\u001b[0m modified_img \u001b[39m=\u001b[39m modified_img\u001b[39m.\u001b[39mrotate(random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m360\u001b[39m))  \u001b[39m# Rotate the image randomly\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Change the color by adjusting the brightness, contrast, and saturation\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m brightness \u001b[39m=\u001b[39m ImageEnhance\u001b[39m.\u001b[39mBrightness(modified_img)\n\u001b[0;32m     39\u001b[0m modified_img \u001b[39m=\u001b[39m brightness\u001b[39m.\u001b[39menhance(random\u001b[39m.\u001b[39muniform(\u001b[39m0.5\u001b[39m, \u001b[39m1.5\u001b[39m))\n\u001b[0;32m     41\u001b[0m contrast \u001b[39m=\u001b[39m ImageEnhance\u001b[39m.\u001b[39mContrast(modified_img)\n","\u001b[1;31mNameError\u001b[0m: name 'ImageEnhance' is not defined"]}],"source":["for iter in range(epochs):\n","    start_time = time.time()  # 记录开始时间\n","    m = max(int(frac * num_users), 1)\n","    idxs_users = np.random.choice(range(num_users), m, replace=False)  # ，replace=False表示不允许重复选择。\n","    w_locals_client = []  # 用于存储每个客户端训练后的本地模型参数。\n","    loss_avg_test_all_dict = {}\n","    acc_avg_test_all_dict = {}\n","    idx_local_client_dict = {}\n","\n","    for idx in idxs_users:\n","        is_attacker = idx in attackers_indices\n","        local = Client(net_glob_client, idx, lr, device, dataset_train=dataset_train, dataset_test=dataset_test,\n","                       idxs=dict_users[idx], idxs_test=dict_users_test[idx], is_attacker=is_attacker)\n","        # Training ------------------\n","        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n","        idx_local_client_dict[len(w_locals_client)] = idx\n","        w_locals_client.append(copy.deepcopy(w_client))\n","\n","        # Testing -------------------\n","        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter, selected_clients=best_clients_indices)\n","\n","        # Update the dictionaries with the client's self.loss_avg_test_all and self.acc_avg_test_all\n","        loss_avg_test_all_dict[idx] = local.loss_avg_test_all\n","        acc_avg_test_all_dict[idx] = local.acc_avg_test_all\n","\n","    print(\"idxs_users\", idxs_users)\n","    # Ater serving all clients for its local epochs------------\n","    # Federation process at Client-Side------------------------\n","    print(\"------------------------------------------------------------\")\n","    print(\"------ Fed Server: Federation process at Client-Side -------\")\n","    print(\"------------------------------------------------------------\")\n","    # w_locals_client是所有客户端训练后的本地模型参数列表，FedAvg函数是加权平均函数，返回全局模型参数w_glob_client。\n","    # w_glob_client = FedAvg(w_locals_client)\n","    # 调用 Krum 算法\n","    num_to_select = int(num_users * (1 - poisoned_frac - 0.1))  # 选择的客户端数量\n","    w_glob_client, best_clients_indices = krum_aggregation(w_locals_client, num_to_select)\n","    print([idxs_users[i] for i in best_clients_indices])\n","    # Update client-side global model\n","    net_glob_client.load_state_dict(w_glob_client)\n","\n","    print(\"fedserver选择的客户端index:\", best_clients_indices)\n","    best_clients_idxs = [idx_local_client_dict[i] for i in best_clients_indices]\n","    krum_acc_test_collect_user = [acc_avg_test_all_dict[i] for i in best_clients_idxs]\n","    for acc in krum_acc_test_collect_user:\n","        print(\"acc:\",acc)\n","    krum_loss_test_collect_user = [loss_avg_test_all_dict[i] for i in best_clients_idxs]\n","    \n","    krum_acc_avg_all_user = sum(krum_acc_test_collect_user) / len(krum_acc_test_collect_user)\n","    krum_loss_avg_all_user = sum(krum_loss_test_collect_user) / len(krum_loss_test_collect_user)\n","    krum_acc_test_collect.append(krum_acc_avg_all_user)\n","    krum_loss_test_collect.append(krum_loss_avg_all_user)\n","\n","    print(\"====================== Fed Server==========================\")\n","    print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, acc_avg_all_user_train,\n","                                                                              loss_avg_all_user_train))\n","    print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, krum_acc_avg_all_user,\n","                                                                             krum_loss_avg_all_user))\n","    print(\"==========================================================\")\n","\n","    end_time = time.time()  # 记录结束时间\n","    epoch_time = end_time - start_time  # 计算epoch所耗费的时间\n","    total_time += epoch_time  # 将时间差加到总时间中\n","    # 将时间差值转换为小时、分钟和秒数\n","    hours, rem = divmod(epoch_time, 3600)\n","    minutes, seconds = divmod(rem, 60)\n","    print(f\"Epoch {iter} finished in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n","    print(f\"Epoch {iter} finished. Total time: {total_time:.2f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T11:13:02.197214Z","iopub.status.busy":"2023-04-09T11:13:02.196807Z","iopub.status.idle":"2023-04-09T11:13:02.227737Z","shell.execute_reply":"2023-04-09T11:13:02.226605Z","shell.execute_reply.started":"2023-04-09T11:13:02.197175Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training and Evaluation completed!\n"]}],"source":["# ===================================================================================\n","\n","print(\"Training and Evaluation completed!\")\n","\n","# ===============================================================================\n","# Save output data to .excel file (we use for comparision plots)\n","round_process = [i for i in range(1, len(acc_train_collect) + 1)]\n","df = DataFrame({'round': round_process, 'acc_train': acc_train_collect, 'acc_test': acc_test_collect,\n","                'loss_train': loss_train_collect, 'loss_test': loss_test_collect})\n","krum_df = DataFrame({'round': round_process, 'acc_train': acc_train_collect, 'acc_test': krum_acc_test_collect,\n","                'loss_train': loss_train_collect, 'loss_test': krum_loss_test_collect})\n","file_name = program + \"_\" + dataset_choice + \".xlsx\"\n","krum_df.to_excel(file_name, sheet_name=\"v1_test\", index=False)\n","\n","# =============================================================================\n","#                         Program Completed\n","# ============================================================================="]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}

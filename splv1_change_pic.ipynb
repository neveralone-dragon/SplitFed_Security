{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-11T04:57:45.241651Z","iopub.execute_input":"2023-04-11T04:57:45.242220Z","iopub.status.idle":"2023-04-11T04:57:45.255889Z","shell.execute_reply.started":"2023-04-11T04:57:45.242170Z","shell.execute_reply":"2023-04-11T04:57:45.254854Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 一、预处理\n\n导入相关函数","metadata":{}},{"cell_type":"code","source":"import time\n\n# We have three versions of our implementations\n# Version1: without using socket and no DP+PixelDP\n# Version2: with using socket but no DP+PixelDP\n# Version3: without using socket but with DP+PixelDP\n\n# This program is Version1: Single program simulation\n# ============================================================================\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport math\nimport os.path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom glob import glob\nfrom pandas import DataFrame\nfrom collections import OrderedDict\n\n\nimport torchvision.datasets as datasets\nimport random\nimport numpy as np\nimport os\nfrom torchvision.datasets import ImageFolder\n\nimport matplotlib\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', 1000)\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport copy\n\nSEED = 1234\n\ndef init_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        print(torch.cuda.get_device_name(0))\ninit_seeds(SEED)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:45.258536Z","iopub.execute_input":"2023-04-11T04:57:45.258979Z","iopub.status.idle":"2023-04-11T04:57:46.740127Z","shell.execute_reply.started":"2023-04-11T04:57:45.258940Z","shell.execute_reply":"2023-04-11T04:57:46.739087Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.1模型初始化","metadata":{}},{"cell_type":"code","source":"from torch import nn\nimport torch.nn.functional as F\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\nclass ResNet18_client_side(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(ResNet18_client_side, self).__init__()\n        self.in_planes = 64\n        self.conv1 = conv3x3(3, 64)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet18_server_side(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, pool_size=4):  # Add a new argument pool_size\n        super(ResNet18_server_side, self).__init__()\n        self.in_planes = 64\n        self.pool_size = pool_size  # Add this line to store pool_size\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.layer2(x)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        # print(\"Output shape before pooling:\", out.shape)\n        out = F.avg_pool2d(out, kernel_size=self.pool_size)  # Use self.pool_size instead of 8\n        # print(\"Output shape after pooling:\", out.shape)\n        out = out.view(out.size(0), -1)\n        y_hat = self.linear(out)\n        return y_hat","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:46.741631Z","iopub.execute_input":"2023-04-11T04:57:46.742246Z","iopub.status.idle":"2023-04-11T04:57:46.762400Z","shell.execute_reply.started":"2023-04-11T04:57:46.742204Z","shell.execute_reply":"2023-04-11T04:57:46.761323Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import logging\n\ndef init_logging(program_name):\n    logging.basicConfig(filename=f'{program_name}.log', level=logging.INFO)\n    logging.getLogger().addHandler(logging.StreamHandler())\n    \n# ===================================================================\nprogram = \"SFLV1_ResNet18_base\"\nprint(f\"---------{program}----------\")  # this is to identify the program in the slurm outputs files\ninit_logging(program)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ===================================================================\n\n# No. of users\nnum_users = 20\nepochs = 100\nfrac = 1  # participation of clients; if 1 then 100% clients participate in SFLV1\nlr = 0.0001\n\n# CIFAR10\\HAM10000\ndataset_choice = 'CIFAR10'","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:46.764120Z","iopub.execute_input":"2023-04-11T04:57:46.764480Z","iopub.status.idle":"2023-04-11T04:57:46.775600Z","shell.execute_reply.started":"2023-04-11T04:57:46.764444Z","shell.execute_reply":"2023-04-11T04:57:46.774353Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"---------SFLV1_ResNet18_base----------\n","output_type":"stream"}]},{"cell_type":"code","source":"def init_models(device, dataset_choice):\n    net_glob_client = ResNet18_client_side(BasicBlock, [2, 2, 2, 2]).to(device)\n    if torch.cuda.device_count() > 1:\n        logging.info(f\"We use {torch.cuda.device_count()} GPUs\")\n        net_glob_client = nn.DataParallel(net_glob_client)\n\n    net_glob_client.to(device)\n    logging.info(net_glob_client)\n\n    if dataset_choice == 'HAM10000':\n        num_classes = 7\n        pool_size = 8\n    elif dataset_choice == 'CIFAR10':\n        num_classes = 10\n        pool_size = 4\n    else:\n        raise ValueError('Invalid dataset choice.')\n\n    net_glob_server = ResNet18_server_side(BasicBlock, [2,2,2,2], num_classes=num_classes,pool_size=pool_size)\n    if torch.cuda.device_count() > 1:\n        logging.info(f\"We use {torch.cuda.device_count()} GPUs\")\n        net_glob_server = nn.DataParallel(net_glob_server)\n\n    net_glob_server.to(device)\n    logging.info(net_glob_server)\n\n    return net_glob_client, net_glob_server\n\nnet_glob_client, net_glob_server = init_models(device, dataset_choice)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-11T04:57:46.779353Z","iopub.execute_input":"2023-04-11T04:57:46.779893Z","iopub.status.idle":"2023-04-11T04:57:48.335978Z","shell.execute_reply.started":"2023-04-11T04:57:46.779855Z","shell.execute_reply":"2023-04-11T04:57:48.334800Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"ResNet18_client_side(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n)\nResNet18_server_side(\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2 变量初始化","metadata":{}},{"cell_type":"code","source":"# ===================================================================================\n# For Server Side Loss and Accuracy\nloss_train_collect = []\nacc_train_collect = []\nloss_test_collect = []\nacc_test_collect = []\nbatch_acc_train = []\nbatch_loss_train = []\nbatch_acc_test = []\nbatch_loss_test = []\n\ncriterion = nn.CrossEntropyLoss()\ncount1 = 0\ncount2 = 0\n\n# ====================================================================================================\n#                                  Server Side Program\n# ====================================================================================================\n# Federated averaging: FedAvg\n# to print train - test together in each round-- these are made global\nacc_avg_all_user_train = 0\nloss_avg_all_user_train = 0\nloss_train_collect_user = []\nacc_train_collect_user = []\nloss_test_collect_user = []\nacc_test_collect_user = []\n\n# （即权重和偏置）保存到w_glob_server中。\nw_glob_server = net_glob_server.state_dict()\nw_locals_server = []\n\n# client idx collector\nidx_collect = []  # 初始化一个空列表，用于收集选择的客户端的索引。\nl_epoch_check = False  # 初始化一个布尔变量，用于指示是否进行了本地训练轮次的检查。\nfed_check = False  # 初始化一个布尔变量，用于指示是否完成了联邦学习。\n# Initialization of net_model_server and net_server (server-side model)\nnet_model_server = [net_glob_server for i in range(num_users)]  # 该列表包含了每个客户端的初始模型。\nnet_server = copy.deepcopy(net_model_server[0]).to(device)  # 初始化为net_model_server的第一个元素的深拷贝，并将其移到GPU上。\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:48.337687Z","iopub.execute_input":"2023-04-11T04:57:48.338666Z","iopub.status.idle":"2023-04-11T04:57:48.361785Z","shell.execute_reply.started":"2023-04-11T04:57:48.338623Z","shell.execute_reply":"2023-04-11T04:57:48.360872Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 1.3定义服务端","metadata":{}},{"cell_type":"code","source":"def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n    \"\"\"\n\n    Args:\n        fx_client: 一个函数，用于在客户端更新模型参数，它接受以下参数：net_model_client（客户端模型），optimizer_client（客户端优化器），train_loader（客户端训练数据），l_epoch（客户端训练轮数）。\n        y:目标变量的标签值。\n        l_epoch_count:训练的总轮数\n        l_epoch:当前训练的轮数\n        idx:用于选择在全局模型中使用哪些本地模型进行更新的客户端的索引。\n        len_batch:训练数据的批次大小。\n\n    Returns:\n\n    \"\"\"\n    # 这些是全局变量，因为它们在函数内被更新，并且在函数之外被调用。\n    \"\"\"\n    net_model_server: 全局模型。\n    criterion: 损失函数，用于计算模型的误差。\n    optimizer_server: 优化器，用于更新全局模型的参数。\n    device: 设备（CPU或GPU）用于计算。\n    batch_acc_train: 当前批次的准确度。\n    batch_loss_train: 当前批次的损失。\n    l_epoch_check: 在训练期间用于检查损失和准确度的训练周期数。\n    fed_check: 用于检查训练周期是否已完成的标志。\n    loss_train_collect: 用于收集所有客户端训练损失的列表。\n    acc_train_collect: 用于收集所有客户端训练准确度的列表。\n    count1: 计数器，用于跟踪当前已经训练的客户端数量。\n    acc_avg_all_user_train: 所有客户端训练准确度的平均值。\n    loss_avg_all_user_train: 所有客户端训练损失的平均值。\n    idx_collect: 用于跟踪已经训练的客户端的索引列表。\n    w_locals_server: 所有客户端本地模型参数的列表。\n    w_glob_server: 全局模型参数的列表。\n    net_server: 全局模型。\n    \"\"\"\n    global net_model_server, criterion, optimizer_server, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect, w_locals_server, w_glob_server, net_server\n    global loss_train_collect_user, acc_train_collect_user, lr\n\n    # net_server是全局模型，返回制定索引的本地模型\n    net_server = copy.deepcopy(net_model_server[idx]).to(\n        device)  # copy.deepcopy() 函数用于创建一个当前本地模型的副本，以便我们可以在全局模型的更新过程中使用它，而不会对原始本地模型进行更改。\n    # 方法将模型设置为训练模式，这意味着在计算时会使用训练期间的正则化技术，如dropout或batch normalization。\n    net_server.train()\n    # 是一个PyTorch中的Adam优化器的实现，它接受模型参数和学习率作为参数，用于更新模型参数以最小化损失函数。在这里，我们使用全局模型的参数和一个预定义的学习率 lr 创建了一个Adam优化器对象\n    optimizer_server = torch.optim.Adam(net_server.parameters(), lr=lr)\n\n    # 1.train and update\n    # 用于清空之前的梯度信息，这样我们可以在每个训练迭代中计算新的梯度并更新模型参数。\n    optimizer_server.zero_grad()\n\n    fx_client = fx_client.to(device)\n    y = y.to(device)\n\n    # ---------forward prop-------------\n    fx_server = net_server(fx_client)  # 作为输入传递到全局模型 net_server 中，然后返回模型的预测输出 fx_server\n\n    # calculate loss\n    loss = criterion(fx_server, y)\n    # calculate accuracy\n    acc = calculate_accuracy(fx_server, y)\n\n    # --------backward prop--------------\n    loss.backward()\n    # 由于我们需要在全局模型更新之前将 fx_client 更新到最新的版本，因此我们使用 clone().detach() 函数来创建一个新的 dfx_client 张量，它具有相同的值但不会被计算图所记录。\n    dfx_client = fx_client.grad.clone().detach()\n    optimizer_server.step()\n\n    batch_loss_train.append(loss.item())\n    batch_acc_train.append(acc.item())\n\n    # Update the server-side model for the current batch\n    net_model_server[idx] = copy.deepcopy(net_server)\n\n    # count1: to track the completion of the local batch associated with one client\n    count1 += 1\n    if count1 == len_batch:\n        acc_avg_train = sum(batch_acc_train) / len(batch_acc_train)  # 计算当前batch的准确率\n        loss_avg_train = sum(batch_loss_train) / len(batch_loss_train)  # 计算当前batch的损失\n\n        batch_acc_train = []  # 将当前batch准确率清零\n        batch_loss_train = []\n        count1 = 0\n\n        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train,\n                                                                                      loss_avg_train))\n\n        # copy the last trained model in the batch\n        # 的状态字典复制到一个新的字典中，以便我们可以将其发送到参与者，从而启动下一轮的联邦学习。注意，w_server 中包含的参数是最新一轮训练的参数，因此每个参与者将从这些参数开始训练它们的本地模型。\n        w_server = net_server.state_dict()\n\n        # If one local epoch is completed, after this a new client will come\n        if l_epoch_count == l_epoch - 1:\n            # l_epoch_count 是本地epoch的计数器，l_epoch 是本地epoch的总数。当计数器 l_epoch_count 等于总数 l_epoch 减 1 时，说明本地epoch已经完成。\n            # # 标记已经完成本地epoch\n            l_epoch_check = True  # to evaluate_server function - to check local epoch has completed or not\n            # We store the state of the net_glob_server()\n            # w_server 是全局模型中最新的训练参数，w_locals_server 是用于存储每个参与者的最后一轮训练参数的列表。因此，当本地epoch完成时，将 w_server 添加到 w_locals_server 中，以便之后将其发送到联邦平均服务器。\n            w_locals_server.append(copy.deepcopy(w_server))\n\n            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n            # this is because we work on the last trained model and its accuracy (not earlier cases)\n\n            # print(\"accuracy = \", acc_avg_train)\n            acc_avg_train_all = acc_avg_train  # 记录最后一个batch的准确率和损失，作为本地epoch的结果\n            loss_avg_train_all = loss_avg_train  #\n\n            # accumulate accuracy and loss for each new user\n            loss_train_collect_user.append(loss_avg_train_all)  # 将本地epoch的损失添加到损失列表中\n            acc_train_collect_user.append(acc_avg_train_all)  # # 将本地epoch的准确率添加到准确率列表中\n\n            # collect the id of each new user\n            if idx not in idx_collect:\n                idx_collect.append(idx)\n                # print(idx_collect)\n#                 print(\"已经训练的客户端:\" + str(idx_collect))\n\n        # This is for federation process--------------------\n        if len(idx_collect) == num_users * frac:\n            # 如果客户端编号列表的长度等于客户端总数，说明所有客户端的训练结果都已经到达服务器了。\n            # 这里不对，是选择的客户端总数\n            fed_check = True  # to evaluate_server function  - to check fed check has hitted\n\n            # 异常检测\n            # 使用异常检测移除异常客户端\n            # anomaly_threshold = 2  # 自定义阈值\n            # w_locals_server = remove_anomalies(w_locals_server, w_glob_server.state_dict(), anomaly_threshold)\n\n            # ================== 使用不同的聚合算法 =================\n            w_glob_server = FedAvg(w_locals_server)  # 使用联邦平均算法更新全局模型，将所有客户端的本地模型参数传入该函数中。\n#             w_glob_server, selected_clients_indices = krum_aggregation(w_locals_server, 3)\n\n            # server-side global model update and distribute that model to all clients ------------------------------\n            net_glob_server.load_state_dict(w_glob_server)  # 将更新后的全局模型参数加载到服务器端的模型中。\n            net_model_server = [net_glob_server for i in\n                                range(num_users)]  # 创建一个长度为客户端数量的列表，每个元素都是更新后的全局模型。这个列表用于向每个客户端分发全局模型参数。\n\n\n\n            acc_avg_all_user_train = sum(acc_train_collect_user) / len(acc_train_collect_user)  # 计算所有客户端训练结果的平均准确率和损失\n            loss_avg_all_user_train = sum(loss_train_collect_user) / len(loss_train_collect_user)\n\n            # 更新性能指标列表\n            loss_train_collect.append(loss_avg_all_user_train)\n            acc_train_collect.append(acc_avg_all_user_train)\n\n            acc_train_collect_user = []\n            loss_train_collect_user = []\n\n            w_locals_server = []  # # 清空本地模型参数列表\n            idx_collect = []  # 清空客户端编号列表\n\n    # send gradients to the client\n    return dfx_client\n\n\ndef evaluate_server(fx_client, y, idx, len_batch, ell, selected_clients):\n    global net_model_server, criterion, batch_acc_test, batch_loss_test, check_fed, net_server, net_glob_server\n    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, w_glob_server, l_epoch_check, fed_check\n    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n\n    net = copy.deepcopy(net_model_server[idx]).to(device)\n    net.eval()\n    return_local_results = False\n\n    with torch.no_grad():\n        # with torch.no_grad()是一个上下文管理器，它可以暂时关闭所有的requires_grad标志，从而不计算梯度1。这样可以节省内存，提高推理速度，也可以避免不必要的梯度累积2。通常在验证或部署模型时使用这个方法3。\n        fx_client = fx_client.to(device)\n        y = y.to(device)\n        # ---------forward prop-------------\n        fx_server = net(fx_client)\n\n        # calculate loss\n        loss = criterion(fx_server, y)\n        # calculate accuracy\n        acc = calculate_accuracy(fx_server, y)\n\n        batch_loss_test.append(loss.item())\n        batch_acc_test.append(acc.item())\n\n        count2 += 1\n        if count2 == len_batch:\n            acc_avg_test = sum(batch_acc_test) / len(batch_acc_test)\n            loss_avg_test = sum(batch_loss_test) / len(batch_loss_test)\n\n            batch_acc_test = []\n            batch_loss_test = []\n            count2 = 0\n\n            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test,\n                                                                                             loss_avg_test))\n\n            # if a local epoch is completed\n            if l_epoch_check:\n                l_epoch_check = False\n                return_local_results = True\n\n                # Store the last accuracy and loss\n                acc_avg_test_all = acc_avg_test\n                loss_avg_test_all = loss_avg_test\n\n                loss_test_collect_user.append(loss_avg_test_all)\n                acc_test_collect_user.append(acc_avg_test_all)\n\n            # if federation is happened----------\n            if fed_check:\n                fed_check = False\n                print(\"------------------------------------------------\")\n                print(\"------ Federation process at Server-Side ------- \")\n                print(\"------------------------------------------------\")\n\n                # 计算Krum选定客户端的平均准确率和损失\n                if selected_clients is None or len(selected_clients) == 0:\n                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n                else:\n                    print(\"选择的客户端index:\", selected_clients)\n                    acc_test_collect_user = [acc_test_collect_user[i] for i in selected_clients]\n                    loss_test_collect_user = [loss_test_collect_user[i] for i in selected_clients]\n\n                    acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n                    loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n\n                loss_test_collect.append(loss_avg_all_user)\n                acc_test_collect.append(acc_avg_all_user)\n                acc_test_collect_user = []\n                loss_test_collect_user = []\n\n                print(\"====================== SERVER V1==========================\")\n                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train,\n                                                                                          loss_avg_all_user_train))\n                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user,\n                                                                                         loss_avg_all_user))\n                print(\"==========================================================\")\n\n    if return_local_results:\n        return acc_avg_test_all, loss_avg_test_all\n    else:\n        return None, None\n\n\nclass Client(object):\n    def __init__(self, net_client_model, idx, lr, device, dataset_train=None, dataset_test=None, idxs=None,\n                 idxs_test=None):\n        # net_client_model:一个与客户端实例相关的神经网络模型。\n        self.idx = idx  # 一个整数，表示客户端的索引\n        self.device = device  # 一个字符串，表示执行客户端计算的设备。\n        self.lr = lr\n        self.local_ep = 1\n        # self.selected_clients = []\n        # DatasetSplit(dataset_train, idxs)表示使用DatasetSplit类将原始的数据集dataset_train按照索引idxs进行划分，以获得当前客户端可用于训练的数据集。\n        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size=128,\n                                    shuffle=True)  # 一个PyTorch数据集，表示客户端可用于训练的数据\n        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size=128, shuffle=True)\n\n    def train(self, net):\n        net.train()\n        optimizer_client = torch.optim.Adam(net.parameters(), lr=self.lr)\n\n        for iter in range(self.local_ep):\n            # 外层循环是客户端的本地训练轮数self.local_ep\n            len_batch = len(self.ldr_train)\n            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n                # 内层循环是数据加载器self.ldr_train中每个批次的训练。在每个批次中，将图像和标签加载到设备上，然后将优化器的梯度清零。\n                images, labels = images.to(self.device), labels.to(self.device)\n                optimizer_client.zero_grad()\n                # ---------forward prop-------------\n                fx = net(images)\n                # 生成一个可求导的副本client_fx\n                client_fx = fx.clone().detach().requires_grad_(True)\n\n                # Sending activations to server and receiving gradients from server\n                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n\n                # --------backward prop -------------\n                fx.backward(dfx)\n                optimizer_client.step()\n\n            # prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n\n        return net.state_dict()\n\n    def evaluate(self, net, ell, selected_clients=None):\n        net.eval()\n\n        with torch.no_grad():\n            len_batch = len(self.ldr_test)\n            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n                images, labels = images.to(self.device), labels.to(self.device)\n                # ---------forward prop-------------\n                fx = net(images)\n\n                # Sending activations to server\n                acc_avg_test_all, loss_avg_test_all = evaluate_server(fx, labels, self.idx, len_batch, ell, selected_clients)\n\n            # prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n            if loss_avg_test_all is not None and acc_avg_test_all is not None:\n                self.loss_avg_test_all = loss_avg_test_all\n                self.acc_avg_test_all = acc_avg_test_all\n\n        return","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:48.363520Z","iopub.execute_input":"2023-04-11T04:57:48.364003Z","iopub.status.idle":"2023-04-11T04:57:48.403107Z","shell.execute_reply.started":"2023-04-11T04:57:48.363965Z","shell.execute_reply":"2023-04-11T04:57:48.401763Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 2.Data Loading","metadata":{}},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass DatasetSplit(Dataset):\n    def __init__(self, dataset, idxs):\n        self.dataset = dataset\n        self.idxs = list(idxs)\n\n    def __len__(self):\n        return len(self.idxs)\n\n    def __getitem__(self, item):\n        image, label = self.dataset[self.idxs[item]]\n        return image, label\n\n\n# ==============================================================\n# Custom dataset prepration in Pytorch format\nclass SkinData(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        X = Image.open(self.df['path'][index]).resize((64, 64))\n        y = torch.tensor(int(self.df['target'][index]))\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n\ndef dataset_iid(dataset, num_users):\n    \"\"\"\n    该函数接受一个数据集dataset和一个整数num_users作为输入。\n    它的作用是将数据集分割成num_users份，以便每个客户端都有一份相同分布的数据集。\n    :param dataset:\n    :param num_users:\n    :return:函数返回一个字典dict_users，其中包含num_users个键，每个键对应一个客户端，值为该客户端所分配的数据集索引的集合。\n    dict_users:{idx:int : []:list}\n    \"\"\"\n    # 该函数首先计算每个客户端应该拥有的数据量num_items\n    num_items = int(len(dataset)/num_users)\n    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        # 接着，函数使用np.random.choice函数从all_idxs中选择num_items个索引，将这些索引添加到字典dict_users的第i个键中，表示第i个客户端的数据集。在选择后，从all_idxs中移除已经分配给第i个客户端的索引。\n        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n        all_idxs = list(set(all_idxs) - dict_users[i])\n    return dict_users\n\ndef load_data(dataset_choice, num_users):\n    \"\"\"\n\n    :param dataset_choice: 选择的数据集\n    :param num_users: 用户的数量\n    :return:\n    \"\"\"\n    if dataset_choice == 'HAM10000':\n        df = pd.read_csv('data/HAM10000_metadata.csv')\n        print(df.head())\n\n        lesion_type = {\n            'nv': 'Melanocytic nevi',\n            'mel': 'Melanoma',\n            'bkl': 'Benign keratosis-like lesions ',\n            'bcc': 'Basal cell carcinoma',\n            'akiec': 'Actinic keratoses',\n            'vasc': 'Vascular lesions',\n            'df': 'Dermatofibroma'\n        }\n\n        # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n        imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n                        for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n\n        # print(\"path---------------------------------------\", imageid_path.get)\n        # 将图像id映射为图像文件的路径，并将其存储在数据集中的path列中。\n        df['path'] = df['image_id'].map(imageid_path.get)\n        # 将诊断编码映射为对应的分类名称，并将其存储在数据集中的cell_type列中。\n        df['cell_type'] = df['dx'].map(lesion_type.get)\n        # 将分类名称转换为数字编码，并将其存储在数据集中的target列中。这里使用了.\n        # 可以将字符串类型的分类变量转换为数字编码，其中不同的分类名称对应不同的数字编码。\n        df['target'] = pd.Categorical(df['cell_type']).codes\n        print(df['cell_type'].value_counts())\n        print(df['target'].value_counts())\n\n        # =============================================================================\n        # Train-test split\n        train, test = train_test_split(df, test_size=0.2)\n\n        train = train.reset_index()\n\n        test = test.reset_index()\n\n        # =============================================================================\n        #                         Data preprocessing\n        # =============================================================================\n        # Data preprocessing: Transformation\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                               transforms.RandomVerticalFlip(),\n                                               transforms.Pad(3),\n                                               transforms.RandomRotation(10),\n                                               transforms.CenterCrop(64),\n                                               transforms.ToTensor(),\n                                               transforms.Normalize(mean=mean, std=std)\n                                               ])\n\n        test_transforms = transforms.Compose([\n            transforms.Pad(3),\n            transforms.CenterCrop(64),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n        # With augmentation\n        dataset_train = SkinData(train, transform=train_transforms)\n        dataset_test = SkinData(test, transform=test_transforms)\n\n        # ----------------------------------------------------------------\n        dict_users = dataset_iid(dataset_train, num_users)\n        dict_users_test = dataset_iid(dataset_test, num_users)\n    elif dataset_choice == 'CIFAR10':\n        # =============================================================================\n        #                         Data loading\n        # =============================================================================\n        # Load CIFAR-10 dataset\n        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n        testset = datasets.CIFAR10(root='./data', train=False, download=True)\n\n        train_df = pd.DataFrame(trainset.targets, columns=['target'])\n        test_df = pd.DataFrame(testset.targets, columns=['target'])\n\n        # Set the class names\n        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n        train_df['cell_type'] = train_df['target'].apply(lambda x: class_names[x])\n        test_df['cell_type'] = test_df['target'].apply(lambda x: class_names[x])\n\n        print(train_df['cell_type'].value_counts())\n        print(train_df['target'].value_counts())\n\n        # =============================================================================\n        # Train-test split\n        train = train_df.reset_index()\n        test = test_df.reset_index()\n\n        # =============================================================================\n        #                         Data preprocessing\n        # =============================================================================\n        # Data preprocessing: Transformation\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                               transforms.RandomVerticalFlip(),\n                                               transforms.Pad(3),\n                                               transforms.RandomRotation(10),\n                                               transforms.CenterCrop(32),\n                                               transforms.ToTensor(),\n                                               transforms.Normalize(mean=mean, std=std)\n                                               ])\n\n        test_transforms = transforms.Compose([\n            transforms.Pad(3),\n            transforms.CenterCrop(32),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n        # With augmentation\n        dataset_train = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n        dataset_test = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n\n        # ----------------------------------------------------------------\n        dict_users = dataset_iid(dataset_train, num_users)\n        dict_users_test = dataset_iid(dataset_test, num_users)\n    else:\n        raise ValueError(\"Invalid dataset_choice: Choose either 'HAM10000' or 'CIFAR10'\")\n\n    return dataset_train,dataset_test,dict_users,dict_users_test","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:48.404713Z","iopub.execute_input":"2023-04-11T04:57:48.405323Z","iopub.status.idle":"2023-04-11T04:57:48.431631Z","shell.execute_reply.started":"2023-04-11T04:57:48.405287Z","shell.execute_reply":"2023-04-11T04:57:48.430662Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset_train, dataset_test, dict_users, dict_users_test = load_data(dataset_choice, num_users)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:48.432977Z","iopub.execute_input":"2023-04-11T04:57:48.433367Z","iopub.status.idle":"2023-04-11T04:57:51.789872Z","shell.execute_reply.started":"2023-04-11T04:57:48.433339Z","shell.execute_reply":"2023-04-11T04:57:51.788817Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nfrog          5000\ntruck         5000\ndeer          5000\nautomobile    5000\nbird          5000\nhorse         5000\nship          5000\ncat           5000\ndog           5000\nairplane      5000\nName: cell_type, dtype: int64\n6    5000\n9    5000\n4    5000\n1    5000\n2    5000\n7    5000\n8    5000\n3    5000\n5    5000\n0    5000\nName: target, dtype: int64\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.2 聚合算法","metadata":{}},{"cell_type":"code","source":"# To print in color -------test/train of the client side\ndef prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\ndef prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n\ndef calculate_accuracy(fx, y):\n    preds = fx.max(1, keepdim=True)[1]\n    correct = preds.eq(y.view_as(preds)).sum()\n    acc = 100.00 *correct.float()/preds.shape[0]\n    return acc\n\n\n# Federated averaging: FedAvg\ndef FedAvg(w):\n    w_avg = copy.deepcopy(w[0])\n    for k in w_avg.keys():\n        for i in range(1, len(w)):\n            w_avg[k] += w[i][k]\n        w_avg[k] = torch.div(w_avg[k], len(w))\n    return w_avg\n\n\ndef FedAvg_with_grad_clipping(w_locals, clip_value=1):\n    w_avg = copy.deepcopy(w_locals[0])\n    for k in w_avg.keys():\n        for i in range(1, len(w_locals)):\n            w_avg[k] += w_locals[i][k]\n\n        w_avg[k] = torch.div(w_avg[k], len(w_locals))\n\n        # 梯度裁剪\n        w_avg[k] = torch.clamp(w_avg[k], -clip_value, clip_value)\n\n    return w_avg\n\n\n\ndef FedMedian(w_locals):\n    w_median = {}\n    for k in w_locals[0].keys():\n        w_median[k] = torch.median(torch.stack([w[k] for w in w_locals]), dim=0).values\n    return w_median\n\ndef trimmed_and_thresholded_aggregation(weight_diffs, trim_ratio=0.1, threshold=0.01):\n    \"\"\"\n    :param weight_diffs: 客户端权重差列表，每个元素是一个客户端的权重差\n    :param trim_ratio: 用于权重修剪的比例,例如，当 trim_ratio=0.1 时，表示要剔除权重中绝对值最大的 10%。\n    :param threshold: 用于阈值聚合的阈值,表示阈值聚合时的阈值。只有权重差的绝对值小于这个阈值的权重才会被聚合。\n    :return:\n    \"\"\"\n\n    # 对权重差进行修剪\n    trimmed_weight_diffs = []\n    for diff in weight_diffs:\n        trimmed_diff = {}\n        for key in diff.keys():\n            # 然后，对于权重差中的每个键（层），代码将该键对应的权重差提取出来\n            layer_diff = diff[key]\n            layer_abs_diff = torch.abs(layer_diff)\n            sorted_indices = torch.argsort(layer_abs_diff.view(-1), descending=True)\n            trim_index = int(len(sorted_indices) * (1 - trim_ratio))\n            indices_to_keep = sorted_indices[:trim_index]\n            mask = torch.zeros_like(layer_diff, dtype=torch.bool)\n            mask.view(-1)[indices_to_keep] = True\n            trimmed_layer_diff = torch.where(mask, layer_diff, torch.zeros_like(layer_diff))\n            trimmed_diff[key] = trimmed_layer_diff\n        trimmed_weight_diffs.append(trimmed_diff)\n\n    # 对修剪后的权重差进行阈值聚合\n    # 初始化一个空字典 aggregated_weight_diff，用于存储阈值聚合后的权重差。\n    aggregated_weight_diff = {}\n    for i, weight_diff in enumerate(trimmed_weight_diffs):\n        for k, v in weight_diff.items():\n            if i == 0:\n                # 对于第一个权重差，将其除以客户端数量并存储在 aggregated_weight_diff 中。\n                aggregated_weight_diff[k] = v / float(len(trimmed_weight_diffs))\n            else:\n                # 对于其他权重差，如果其绝对值小于阈值 threshold，则将其除以客户端数量并累加到 aggregated_weight_diff 对应的键中。\n                mask = torch.abs(v) < threshold\n                aggregated_weight_diff[k] += torch.where(mask, v / float(len(trimmed_weight_diffs)), torch.zeros_like(v, dtype=torch.float))\n\n    return aggregated_weight_diff\n\n\n\n\ndef subtract_weights(w1, w2):\n    \"\"\"\n    计算 w1 和 w2 之间的差，即 w1 - w2。\n\n    Args:\n        w1 (OrderedDict): 权重字典 1\n        w2 (OrderedDict): 权重字典 2\n\n    Returns:\n        OrderedDict: w1 和 w2 之间的权重差异\n    \"\"\"\n    diff = OrderedDict()\n    for key in w1.keys():\n        diff[key] = w1[key] - w2[key]\n    return diff\n\n\ndef add_weights(w1, w2):\n    \"\"\"\n    对 w1 和 w2 进行加法操作，即 w1 + w2。\n\n    Args:\n        w1 (OrderedDict): 权重字典 1\n        w2 (OrderedDict): 权重字典 2\n\n    Returns:\n        OrderedDict: w1 和 w2 相加的结果\n    \"\"\"\n    added = OrderedDict()\n    for key in w1.keys():\n        added[key] = w1[key] + w2[key]\n    return added\n\n\n\ndef remove_anomalies(w_locals, w_glob, threshold):\n    # 异常检测\n    anomaly_indices = detect_anomaly(w_locals, w_glob, threshold)\n    # 删除异常客户端的模型更新\n    w_locals = [w for idx, w in enumerate(w_locals) if idx not in anomaly_indices]\n    return w_locals\n\n\ndef detect_anomaly(w_locals, w_glob_client,threshold):\n    grad_norms = []\n    # 计算所有客户端模型更新的梯度范数（与全局模型之间的差异）\n    for idx, w in enumerate(w_locals):\n        grad_norm = torch.norm(subtract_weights(w, w_glob_client), p=2)\n        grad_norms.append(grad_norm.item())\n\n    # 计算梯度范数的标准差和平均值\n    std_dev = np.std(grad_norms)\n    mean_grad_norm = np.mean(grad_norms)\n    # 确定异常客户端（梯度范数大于平均值加减某个阈值倍的标准差）\n    anomaly_indices = [idx for idx, norm in enumerate(grad_norms) if abs(norm - mean_grad_norm) > threshold * std_dev]\n\n    return anomaly_indices\n\n\ndef krum_aggregation(weight_dicts, num_to_select):\n    # ...（weights_to_array、array_to_weights和pairwise_distances函数保持不变）\n    def weights_to_array(weight_dict):\n        \"\"\"\n        将权重字典转换为 numpy 数组。\n        :param weight_dict: 权重字典，其中每个值都是一个 torch.Tensor 张量。\n        :return: 一个 numpy 数组，其中包含了所有权重张量的扁平化数组。\n        \"\"\"\n        # 初始化 weight_list 列表，用于存储 weight_dict 中每个权重张量的扁平化数组。\n        weight_list = []\n        for key in weight_dict:\n            # 将权重张量转换为 numpy 数组，并使用 flatten 函数将其扁平化\n            weight_list.append(weight_dict[key].cpu().numpy().flatten())\n        # 将 weight_list 中的数组连接成一个 numpy 数组，并返回该数组\n        return np.concatenate(weight_list)\n\n    def array_to_weights(array, weight_dict_template):\n        \"\"\"\n        从 numpy 数组中提取权重张量，并将它们保存到一个新的权重字典中。\n        :param array: 包含所有权重张量的扁平化 numpy 数组。\n        :param weight_dict_template: 一个权重字典模板，其中包含了所有权重张量的形状。\n        :return: 一个新的权重字典，其中包含了从数组中提取的权重张量。\n        \"\"\"\n        # 初始化一个新的有序字典 new_weight_dict，用于存储从 numpy 数组中提取的权重张量\n        new_weight_dict = OrderedDict()\n        # 初始化一个索引变量 idx，用于跟踪从数组中提取权重的位置\n        idx = 0\n        # 遍历权重字典模板中的每个键 key\n        for key in weight_dict_template:\n            # 获取权重张量的大小 size\n            size = weight_dict_template[key].numel()\n            # 从 numpy 数组中提取一个与权重张量相同大小的一维切片，并使用 reshape 函数重新塑形\n            # 将 numpy 数组转换为 torch.Tensor 张量，并将其存储到新的有序字典 new_weight_dict 中\n            new_weight_dict[key] = torch.from_numpy(array[idx:idx + size].reshape(weight_dict_template[key].shape))\n            # 更新索引变量 idx 的值，跳过已经提取的权重张量\n            idx += size\n        # 返回新的有序字典 new_weight_dict\n        return new_weight_dict\n\n    def pairwise_distances(weight_updates):\n        # 获取客户端数量\n        n_clients = len(weight_updates)\n        # 初始化一个距离矩阵，矩阵大小为 n_clients * n_clients\n        distances = np.zeros((n_clients, n_clients))\n\n        # 遍历所有的客户端对\n        for i in range(n_clients):\n            for j in range(i + 1, n_clients):\n                # 计算 i 和 j 客户端之间的欧氏距离\n                dist = np.linalg.norm(weight_updates[i] - weight_updates[j])\n                # 在距离矩阵中记录距离\n                distances[i, j] = dist\n                distances[j, i] = dist\n\n        # 返回距离矩阵\n        return distances\n\n    weight_arrays = [weights_to_array(weight_dict) for weight_dict in weight_dicts]\n    n_clients = len(weight_arrays)\n    distances = pairwise_distances(weight_arrays)\n\n    krum_scores = []\n    for i in range(n_clients):\n        sorted_distances = np.sort(distances[i])\n        # 计算当前客户端的Krum分数，即距离最大的n_clients - num_to_select - 1个客户端之间的距离总和。\n        krum_score = np.sum(sorted_distances[-(n_clients - num_to_select - 1):])\n        krum_scores.append(krum_score)\n\n    # 使用np.argpartition函数找到具有最低Krum分数的num_to_select个客户端的索引。\n    best_clients_indices = np.argpartition(krum_scores, num_to_select)[:num_to_select]\n\n    # 从权重数组中选择最佳客户端的权重。\n    selected_weight_arrays = [weight_arrays[i] for i in best_clients_indices]\n    # 计算选定客户端的权重数组的平均值，得到聚合后的权重数组。\n    aggregated_weight_array = np.mean(selected_weight_arrays, axis=0)\n\n    # 使用array_to_weights函数将聚合后的权重数组转换回权重字典。\n    aggregated_weights = array_to_weights(aggregated_weight_array, weight_dicts[0])\n\n    # 返回聚合后的权重字典和最好的客户端索引列表。\n    return aggregated_weights, best_clients_indices","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:51.791796Z","iopub.execute_input":"2023-04-11T04:57:51.792235Z","iopub.status.idle":"2023-04-11T04:57:51.825614Z","shell.execute_reply.started":"2023-04-11T04:57:51.792191Z","shell.execute_reply":"2023-04-11T04:57:51.824546Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 2.3数据投毒——标签翻转","metadata":{}},{"cell_type":"code","source":"def replace_label1_with_label2_on_df(df,label1,label2,poisoned_dict_users):\n    \"\"\"\n    标签反转\n    :param df:dataframe\n    :param label1: 等待翻转的标签\n    :param label2: 需要翻转的标签\n    :param poisoned_dict_users:包含索引列表的字典,中毒的用户\n    :return:\n    \"\"\"\n    for idx_list in poisoned_dict_users.values():\n        for idx in idx_list:\n            if df.loc[idx,'target'] == label1:\n                df.loc[idx,'target'] = label2\n    # df.loc[df['target'] == label1, 'target'] = label2\n    return df\n\ndef random_select_poisoning_users(dict_users, n):\n    \"\"\"\n    随机选择n个key-value对\n    :param dict_users: 字典，key为索引，value为包含索引值的列表\n    :param n: 选择的key-value对的数量\n    :return: 随机选择的key-value对组成的字典\n    \"\"\"\n    selected = {}\n    keys = random.sample(list(dict_users.keys()), n)\n    for k in keys:\n        selected[k] = dict_users[k]\n    return selected\n\n# def poison_data(dataset_train, dict_users, poisoned_users_num, original_label, target_label):\n#     poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n#     replace_label1_with_label2_on_df(dataset_train.df, original_label, target_label, poisoned_dict_users)\n#     return poisoned_dict_users\n\ndef print_poisoning_results(poisoned_dict_users, dataset_train):\n    for poisoned_user_key in poisoned_dict_users:\n        print(\"被投毒的用户:\", poisoned_user_key)\n\n    print(\"标签反转后的target统计:\")\n    print(dataset_train.df['target'].value_counts())\n\ndef cifar10_to_dataframe(dataset):\n    data = [dataset[i] for i in range(len(dataset))]\n    images, labels = zip(*data)\n    df = pd.DataFrame({\"image\": images, \"target\": labels})\n    return df\n\n\n\ndef poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings):\n    if dataset_choice == 'CIFAR10':\n        df_train = cifar10_to_dataframe(dataset_train)\n    elif dataset_choice == 'HAM10000':\n        df_train = dataset_train.df\n    else:\n        raise ValueError(\"Invalid dataset choice.\")\n\n    poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n\n    for original_label, target_label in label_mappings:\n        replace_label1_with_label2_on_df(df_train, original_label, target_label, poisoned_dict_users)\n\n    # 修改 dataset_train 的标签\n    if dataset_choice == 'CIFAR10':\n        for idx, row in df_train.iterrows():\n            dataset_train.targets[idx] = row['target']\n\n    return poisoned_dict_users\n\ndef poison_data_random(dataset_train, dataset_choice, dict_users, poisoned_users_num):\n    if dataset_choice == 'CIFAR10':\n        df_train = cifar10_to_dataframe(dataset_train)\n    elif dataset_choice == 'HAM10000':\n        df_train = dataset_train.df\n    else:\n        raise ValueError(\"Invalid dataset choice.\")\n\n    poisoned_dict_users = random_select_poisoning_users(dict_users, poisoned_users_num)\n\n    for idx_list in poisoned_dict_users.values():\n        for idx in idx_list:\n            labels = list(set(df_train['target'].unique()) - {df_train.loc[idx, 'target']})\n            df_train.loc[idx, 'target'] = random.choice(labels)\n\n    # 修改 dataset_train 的标签\n    if dataset_choice == 'CIFAR10':\n        for idx, row in df_train.iterrows():\n            dataset_train.targets[idx] = row['target']\n\n    return poisoned_dict_users","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:51.828613Z","iopub.execute_input":"2023-04-11T04:57:51.829140Z","iopub.status.idle":"2023-04-11T04:57:51.846744Z","shell.execute_reply.started":"2023-04-11T04:57:51.829111Z","shell.execute_reply":"2023-04-11T04:57:51.845737Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# =============================================================================\n#                         Poisoning\n# =============================================================================\n# 定义攻击模式和目标标签\nattack_pattern = np.full((32, 32, 3), 255, dtype=np.uint8)\ntarget_label = 7\n\n# 增加投毒用户数量至 10%\npoisoned_frac = 0.3\npoisoned_users_num = int(poisoned_frac * num_users)\n# label_mappings = [(4, 2), (1, 7), (3, 1), (2, 6), (6, 5)]\n# poisoned_dict_users = poison_data(dataset_train, dataset_choice, dict_users, poisoned_users_num, label_mappings)\npoisoned_dict_users = poison_data_random(dataset_train, dataset_choice, dict_users, poisoned_users_num)\n# poisoned_dict_users = poison_data_model(dataset_train, dataset_choice, dict_users, poisoned_users_num, attack_pattern, target_label)\n\nfor poisoned_user_key in poisoned_dict_users:\n    print(\"被投毒的用户:\", poisoned_user_key)\n\n# 如果使用的是 HAM10000 数据集，您可以直接使用 dataset_train.df 查看标签分布\nif dataset_choice == 'HAM10000':\n    print(\"标签反转后的target统计:\")\n    print(dataset_train.df['target'].value_counts())\nelif dataset_choice == 'CIFAR10':\n    cifar10_df = cifar10_to_dataframe(dataset_train)\n    print(\"标签反转后的target统计:\")\n    print(cifar10_df['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:57:51.848343Z","iopub.execute_input":"2023-04-11T04:57:51.848735Z","iopub.status.idle":"2023-04-11T04:59:03.912494Z","shell.execute_reply.started":"2023-04-11T04:57:51.848699Z","shell.execute_reply":"2023-04-11T04:59:03.911286Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"被投毒的用户: 14\n被投毒的用户: 3\n被投毒的用户: 0\n被投毒的用户: 2\n被投毒的用户: 1\n被投毒的用户: 10\n标签反转后的target统计:\n9    5079\n2    5072\n1    5031\n4    5019\n7    5000\n5    4999\n6    4987\n3    4984\n8    4919\n0    4910\nName: target, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3.模型train和test\n## 3.1 初始化","metadata":{}},{"cell_type":"code","source":"# ------------ Training And Testing  -----------------\nnet_glob_client.train()\n# copy weights\nw_glob_client = net_glob_client.state_dict()\n# Federation takes place after certain local epochs in train() client-side\n# this epoch is global epoch, also known as rounds\n\ntotal_time = 0.0  # 初始化总时间为0\nbest_clients_indices = None\nkrum_acc_test_collect = []\nkrum_loss_test_collect = []","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:59:03.914353Z","iopub.execute_input":"2023-04-11T04:59:03.914753Z","iopub.status.idle":"2023-04-11T04:59:03.921751Z","shell.execute_reply.started":"2023-04-11T04:59:03.914712Z","shell.execute_reply":"2023-04-11T04:59:03.920619Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for iter in range(epochs):\n    start_time = time.time()  # 记录开始时间\n    m = max(int(frac * num_users), 1)\n    idxs_users = np.random.choice(range(num_users), m, replace=False)  # ，replace=False表示不允许重复选择。\n    w_locals_client = []  # 用于存储每个客户端训练后的本地模型参数。\n    loss_avg_test_all_dict = {}\n    acc_avg_test_all_dict = {}\n    idx_local_client_dict = {}\n\n    for idx in idxs_users:\n#         print(\"正在训练的idx:\",idx)\n        local = Client(net_glob_client, idx, lr, device, dataset_train=dataset_train, dataset_test=dataset_test,\n                       idxs=dict_users[idx], idxs_test=dict_users_test[idx])\n        # Training ------------------\n        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n        idx_local_client_dict[len(w_locals_client)] = idx\n        w_locals_client.append(copy.deepcopy(w_client))\n\n        # Testing -------------------\n        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter, selected_clients=best_clients_indices)\n\n        # Update the dictionaries with the client's self.loss_avg_test_all and self.acc_avg_test_all\n        loss_avg_test_all_dict[idx] = local.loss_avg_test_all\n        acc_avg_test_all_dict[idx] = local.acc_avg_test_all\n\n    print(\"idxs_users\",idxs_users)\n    # Ater serving all clients for its local epochs------------\n    # Federation process at Client-Side------------------------\n    print(\"------------------------------------------------------------\")\n    print(\"------ Fed Server: Federation process at Client-Side -------\")\n    print(\"------------------------------------------------------------\")\n    # w_locals_client是所有客户端训练后的本地模型参数列表，FedAvg函数是加权平均函数，返回全局模型参数w_glob_client。\n    w_glob_client = FedAvg(w_locals_client)\n    # 调用 Krum 算法\n    # num_to_select = int(num_users * (1 - poisoned_frac - 0.1))  # 选择的客户端数量\n    # w_glob_client, best_clients_indices = krum_aggregation(w_locals_client, num_to_select)\n    # print([idxs_users[i] for i in best_clients_indices])\n    # Update client-side global model\n    net_glob_client.load_state_dict(w_glob_client)\n\n    # print(\"fedserver选择的客户端index:\", best_clients_indices)\n    # best_clients_idxs = [idx_local_client_dict[i] for i in best_clients_indices]\n    # krum_acc_test_collect_user = [acc_avg_test_all_dict[i] for i in best_clients_idxs]\n    # for acc in krum_acc_test_collect_user:\n    #     print(\"acc:\",acc)\n    # krum_loss_test_collect_user = [loss_avg_test_all_dict[i] for i in best_clients_idxs]\n    #\n    # krum_acc_avg_all_user = sum(krum_acc_test_collect_user) / len(krum_acc_test_collect_user)\n    # krum_loss_avg_all_user = sum(krum_loss_test_collect_user) / len(krum_loss_test_collect_user)\n    # krum_acc_test_collect.append(krum_acc_avg_all_user)\n    # krum_loss_test_collect.append(krum_loss_avg_all_user)\n\n    # print(\"====================== Fed Server==========================\")\n    # print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, acc_avg_all_user_train,\n    #                                                                           loss_avg_all_user_train))\n    # print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, krum_acc_avg_all_user,\n    #                                                                          krum_loss_avg_all_user))\n    # print(\"==========================================================\")\n\n    end_time = time.time()  # 记录结束时间\n    epoch_time = end_time - start_time  # 计算epoch所耗费的时间\n    total_time += epoch_time  # 将时间差加到总时间中\n    # 将时间差值转换为小时、分钟和秒数\n    hours, rem = divmod(epoch_time, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print(f\"Epoch {iter} finished in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n    print(f\"Epoch {iter} finished. Total time: {total_time:.2f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:59:03.926220Z","iopub.execute_input":"2023-04-11T04:59:03.926495Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 23.727 \tLoss: 2.0410\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 12.433 \tLoss: 3.0269\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 24.733 \tLoss: 2.0394\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 7.860 \tLoss: 3.3002\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.910 \tLoss: 2.3619\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 9.348 \tLoss: 2.3335\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 21.822 \tLoss: 2.0806\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 10.358 \tLoss: 2.9263\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 24.625 \tLoss: 2.0452\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 8.816 \tLoss: 2.9414\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.694 \tLoss: 2.3619\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 6.546 \tLoss: 2.3317\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 23.065 \tLoss: 2.0683\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 7.799 \tLoss: 3.2270\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 22.748 \tLoss: 2.0687\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 9.617 \tLoss: 2.9328\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 23.194 \tLoss: 2.0696\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 9.914 \tLoss: 2.9946\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 22.353 \tLoss: 2.0661\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 10.594 \tLoss: 3.1195\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.319 \tLoss: 2.3591\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 7.624 \tLoss: 2.3364\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 22.693 \tLoss: 2.0895\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 7.974 \tLoss: 2.8398\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.391 \tLoss: 2.3694\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 7.994 \tLoss: 2.3275\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 23.194 \tLoss: 2.0600\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 10.069 \tLoss: 2.8384\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.343 \tLoss: 2.3638\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 9.321 \tLoss: 2.3138\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 23.672 \tLoss: 2.0694\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 9.321 \tLoss: 2.8788\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 22.900 \tLoss: 2.0661\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 12.002 \tLoss: 2.9076\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 10.354 \tLoss: 2.3530\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 9.442 \tLoss: 2.3399\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 22.075 \tLoss: 2.0785\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 10.614 \tLoss: 3.0744\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 22.808 \tLoss: 2.0688\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 10.594 \tLoss: 2.5950\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   0, Avg Accuracy 19.181 | Avg Loss 2.154\n Test: Round   0, Avg Accuracy 9.412 | Avg Loss 2.779\n==========================================================\nidxs_users [11  4 14 18  8  1 12  6 16 17  0  7 10  5  3 13 19  2 15  9]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 0 finished in 00:01:12\nEpoch 0 finished. Total time: 72.02 seconds\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.494 \tLoss: 2.3465\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 8.446 \tLoss: 2.4001\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 30.983 \tLoss: 1.8598\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 12.884 \tLoss: 2.5224\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 29.738 \tLoss: 1.8639\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 12.217 \tLoss: 2.8029\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 31.149 \tLoss: 1.8506\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 13.679 \tLoss: 2.6170\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 30.540 \tLoss: 1.8565\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 17.161 \tLoss: 2.8668\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 30.549 \tLoss: 1.8504\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 12.978 \tLoss: 3.2319\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 30.765 \tLoss: 1.8363\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 12.197 \tLoss: 2.9591\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 29.462 \tLoss: 1.8778\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 15.739 \tLoss: 2.7144\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.327 \tLoss: 2.3451\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 9.463 \tLoss: 2.3288\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 30.227 \tLoss: 1.8643\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 20.622 \tLoss: 2.4955\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 29.752 \tLoss: 1.8684\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 12.217 \tLoss: 3.0070\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.469 \tLoss: 2.3397\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 7.685 \tLoss: 2.4255\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.563 \tLoss: 2.3441\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 10.008 \tLoss: 2.3493\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 31.710 \tLoss: 1.8365\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 12.237 \tLoss: 3.1515\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 29.690 \tLoss: 1.8711\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 12.803 \tLoss: 2.7899\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 29.699 \tLoss: 1.8568\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 14.952 \tLoss: 2.9763\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.451 \tLoss: 2.3454\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 10.655 \tLoss: 2.3491\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 29.648 \tLoss: 1.8558\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 10.554 \tLoss: 2.8630\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 29.800 \tLoss: 1.8297\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 13.564 \tLoss: 2.6933\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.466 \tLoss: 2.3478\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 13.335 \tLoss: 2.4910\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   1, Avg Accuracy 24.074 | Avg Loss 2.002\n Test: Round   1, Avg Accuracy 12.670 | Avg Loss 2.702\n==========================================================\nidxs_users [10  5  4  8 15 16  6  7  3 18 12  1  2 19 13 11  0  9 17 14]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 1 finished in 00:01:11\nEpoch 1 finished. Total time: 143.37 seconds\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 34.602 \tLoss: 1.7678\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 25.700 \tLoss: 1.9688\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 33.323 \tLoss: 1.7750\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 33.661 \tLoss: 1.8127\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 35.352 \tLoss: 1.7641\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 29.357 \tLoss: 1.9571\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.602 \tLoss: 2.3547\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 7.563 \tLoss: 2.4137\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 9.625 \tLoss: 2.3507\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 9.227 \tLoss: 2.3847\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 33.107 \tLoss: 1.7990\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 33.681 \tLoss: 1.8350\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 33.874 \tLoss: 1.7683\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 28.260 \tLoss: 1.9160\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 8.987 \tLoss: 2.3565\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 10.318 \tLoss: 2.3426\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 31.891 \tLoss: 1.7788\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 25.916 \tLoss: 1.9051\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 34.069 \tLoss: 1.7678\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 24.549 \tLoss: 1.9955\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 34.465 \tLoss: 1.7557\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 32.274 \tLoss: 1.8116\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.421 \tLoss: 2.3524\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 11.981 \tLoss: 2.5068\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 32.403 \tLoss: 1.7884\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 28.967 \tLoss: 1.8588\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 32.898 \tLoss: 1.7698\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 35.008 \tLoss: 1.7126\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 34.671 \tLoss: 1.7608\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 28.091 \tLoss: 1.8742\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 34.357 \tLoss: 1.7699\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 25.256 \tLoss: 1.9377\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 33.709 \tLoss: 1.7486\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 29.277 \tLoss: 1.9001\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 32.001 \tLoss: 1.7842\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 25.236 \tLoss: 2.0692\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 8.566 \tLoss: 2.3633\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 10.379 \tLoss: 2.3376\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.959 \tLoss: 2.3550\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 29.418 \tLoss: 1.9509\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   2, Avg Accuracy 26.344 | Avg Loss 1.947\n Test: Round   2, Avg Accuracy 24.206 | Avg Loss 2.025\n==========================================================\nidxs_users [ 4  7 13  2 14  5 15  0 12 17  9 10 16  8 19 11  6 18  3  1]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 2 finished in 00:01:11\nEpoch 2 finished. Total time: 215.01 seconds\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 35.774 \tLoss: 1.7265\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 29.378 \tLoss: 2.0019\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 34.655 \tLoss: 1.7256\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 27.519 \tLoss: 1.8612\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 35.726 \tLoss: 1.7115\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 28.475 \tLoss: 1.9519\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 9.821 \tLoss: 2.3632\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 10.480 \tLoss: 2.3999\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.386 \tLoss: 2.3639\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 10.123 \tLoss: 2.3449\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 35.749 \tLoss: 1.6967\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 29.479 \tLoss: 2.0398\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 34.984 \tLoss: 1.7356\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 30.213 \tLoss: 2.0459\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.963 \tLoss: 2.3558\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 10.379 \tLoss: 2.3557\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 33.488 \tLoss: 1.7171\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 36.106 \tLoss: 1.7007\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 37.192 \tLoss: 1.7134\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 31.513 \tLoss: 1.7658\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.040 \tLoss: 2.3673\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 11.005 \tLoss: 2.3518\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 37.383 \tLoss: 1.6884\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 31.668 \tLoss: 1.8278\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 37.796 \tLoss: 1.6877\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 32.725 \tLoss: 1.7678\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.124 \tLoss: 2.3593\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 6.802 \tLoss: 2.4172\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 36.822 \tLoss: 1.7075\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 29.829 \tLoss: 1.9743\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 35.593 \tLoss: 1.7300\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 29.337 \tLoss: 1.9477\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 36.836 \tLoss: 1.6970\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 31.567 \tLoss: 2.0620\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 34.982 \tLoss: 1.7232\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 34.772 \tLoss: 1.7103\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.007 \tLoss: 2.3613\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 9.887 \tLoss: 2.5413\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 36.236 \tLoss: 1.7035\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 37.527 \tLoss: 1.7926\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   3, Avg Accuracy 28.078 | Avg Loss 1.907\n Test: Round   3, Avg Accuracy 24.939 | Avg Loss 2.043\n==========================================================\nidxs_users [12 16  9 14 10  8 11  2 15  5  3 13 17  1  6  7  4 19  0 18]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 3 finished in 00:01:11\nEpoch 3 finished. Total time: 286.06 seconds\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.274 \tLoss: 2.3672\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 13.955 \tLoss: 2.3815\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 38.773 \tLoss: 1.6778\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 24.589 \tLoss: 2.1938\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 39.506 \tLoss: 1.6473\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 30.691 \tLoss: 2.0012\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 11.117 \tLoss: 2.3550\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 9.638 \tLoss: 2.3739\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 38.633 \tLoss: 1.6518\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 35.298 \tLoss: 1.7462\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 38.054 \tLoss: 1.6696\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 37.379 \tLoss: 1.7910\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 38.578 \tLoss: 1.6679\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 32.671 \tLoss: 1.8248\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 37.316 \tLoss: 1.6742\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 23.532 \tLoss: 2.2674\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 39.384 \tLoss: 1.6559\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 28.031 \tLoss: 1.9965\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 39.655 \tLoss: 1.6413\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 34.085 \tLoss: 1.7219\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.080 \tLoss: 2.3768\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 10.493 \tLoss: 2.3533\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.651 \tLoss: 2.3645\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 11.456 \tLoss: 2.3845\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 39.662 \tLoss: 1.6392\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 31.762 \tLoss: 1.8621\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 37.812 \tLoss: 1.6628\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 35.796 \tLoss: 1.7699\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 36.749 \tLoss: 1.6728\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 34.907 \tLoss: 1.7156\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 37.273 \tLoss: 1.6620\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 19.174 \tLoss: 2.3019\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 38.803 \tLoss: 1.6411\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 31.452 \tLoss: 1.8755\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.414 \tLoss: 2.3657\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 9.207 \tLoss: 2.4528\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 35.827 \tLoss: 1.6634\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 38.894 \tLoss: 1.6992\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.929 \tLoss: 2.3583\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 35.459 \tLoss: 1.7864\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   4, Avg Accuracy 29.824 | Avg Loss 1.871\n Test: Round   4, Avg Accuracy 26.423 | Avg Loss 2.025\n==========================================================\nidxs_users [ 0  7 18  1 16  5  6 15  9 17 10  3 13  4 19 11 12 14  8  2]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 4 finished in 00:01:11\nEpoch 4 finished. Total time: 357.57 seconds\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 40.329 \tLoss: 1.6107\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 33.405 \tLoss: 1.7626\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 39.051 \tLoss: 1.6541\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 36.725 \tLoss: 1.7034\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 39.235 \tLoss: 1.6187\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 30.826 \tLoss: 2.0999\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 39.669 \tLoss: 1.6137\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 26.071 \tLoss: 2.0608\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 40.540 \tLoss: 1.6050\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 36.315 \tLoss: 1.6924\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 39.290 \tLoss: 1.6180\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 32.698 \tLoss: 1.7923\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 40.896 \tLoss: 1.6162\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 39.507 \tLoss: 1.6910\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 39.515 \tLoss: 1.6236\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 30.630 \tLoss: 1.8445\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.051 \tLoss: 2.3616\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 9.341 \tLoss: 2.4148\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.354 \tLoss: 2.3642\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 10.668 \tLoss: 2.3420\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 8.971 \tLoss: 2.3739\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 11.395 \tLoss: 2.3806\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.172 \tLoss: 2.3731\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 12.763 \tLoss: 2.3104\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 39.386 \tLoss: 1.6262\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 40.133 \tLoss: 1.5850\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.768 \tLoss: 2.3690\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 12.763 \tLoss: 2.4611\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 40.014 \tLoss: 1.6008\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 32.718 \tLoss: 1.8147\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 38.943 \tLoss: 1.6182\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 38.860 \tLoss: 1.6260\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 37.668 \tLoss: 1.6396\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 35.217 \tLoss: 1.7548\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 40.317 \tLoss: 1.6256\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 33.486 \tLoss: 1.8835\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 38.513 \tLoss: 1.6284\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 36.180 \tLoss: 1.6822\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.306 \tLoss: 2.3668\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 41.070 \tLoss: 1.7147\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   5, Avg Accuracy 30.649 | Avg Loss 1.845\n Test: Round   5, Avg Accuracy 29.039 | Avg Loss 1.931\n==========================================================\nidxs_users [19  5  6 18 13 11  7  9 14  1 10  3  4  2 17 16 15 12  8  0]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 5 finished in 00:01:11\nEpoch 5 finished. Total time: 429.11 seconds\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 42.093 \tLoss: 1.5705\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 23.909 \tLoss: 2.1151\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 39.393 \tLoss: 1.5960\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 37.082 \tLoss: 1.7371\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 41.923 \tLoss: 1.5970\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 35.163 \tLoss: 1.7143\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 42.826 \tLoss: 1.5706\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 34.786 \tLoss: 1.8662\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 42.245 \tLoss: 1.5756\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 29.236 \tLoss: 2.2197\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 42.112 \tLoss: 1.5907\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 37.783 \tLoss: 1.6599\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.597 \tLoss: 2.3664\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 10.634 \tLoss: 2.4241\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.733 \tLoss: 2.3689\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 11.961 \tLoss: 2.4352\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.625 \tLoss: 2.3788\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 10.735 \tLoss: 2.3649\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 40.935 \tLoss: 1.5920\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 37.298 \tLoss: 1.7025\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.226 \tLoss: 2.3837\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 7.328 \tLoss: 2.3541\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 40.430 \tLoss: 1.5966\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 35.183 \tLoss: 1.8635\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 39.644 \tLoss: 1.5905\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 28.617 \tLoss: 1.9791\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 41.546 \tLoss: 1.5901\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 33.661 \tLoss: 1.7482\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 42.597 \tLoss: 1.5759\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 31.297 \tLoss: 1.7933\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 42.796 \tLoss: 1.5788\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 39.089 \tLoss: 1.7821\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.386 \tLoss: 2.3748\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 12.978 \tLoss: 2.3721\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 9.704 \tLoss: 2.3708\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 13.759 \tLoss: 2.3981\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 39.138 \tLoss: 1.6149\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 35.257 \tLoss: 1.6398\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 41.337 \tLoss: 1.6023\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 43.211 \tLoss: 1.6857\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   6, Avg Accuracy 31.864 | Avg Loss 1.824\n Test: Round   6, Avg Accuracy 27.448 | Avg Loss 1.993\n==========================================================\nidxs_users [12 11  7 13  9  8  1  2  3 18 10  5 15  4 16  6  0 14 19 17]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 6 finished in 00:01:11\nEpoch 6 finished. Total time: 500.60 seconds\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 41.962 \tLoss: 1.5700\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 39.130 \tLoss: 1.6824\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 42.891 \tLoss: 1.5500\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 39.891 \tLoss: 1.6225\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 43.566 \tLoss: 1.5464\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 39.251 \tLoss: 1.7757\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 42.181 \tLoss: 1.5544\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 33.136 \tLoss: 1.8895\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.080 \tLoss: 2.3581\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 8.015 \tLoss: 2.5039\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 43.300 \tLoss: 1.5486\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 33.850 \tLoss: 1.7440\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 43.171 \tLoss: 1.5429\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 32.132 \tLoss: 2.0834\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 42.537 \tLoss: 1.5553\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 31.977 \tLoss: 2.0012\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.060 \tLoss: 2.3795\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 10.789 \tLoss: 2.3680\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 41.703 \tLoss: 1.5839\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 37.076 \tLoss: 1.6692\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.855 \tLoss: 2.3714\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 10.904 \tLoss: 2.3895\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 41.117 \tLoss: 1.5854\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 37.823 \tLoss: 1.7091\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 42.077 \tLoss: 1.5434\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 40.935 \tLoss: 1.5933\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 41.512 \tLoss: 1.5729\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 38.079 \tLoss: 1.7260\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.773 \tLoss: 2.3749\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 12.958 \tLoss: 2.3388\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 42.631 \tLoss: 1.5591\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 38.766 \tLoss: 1.7151\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.030 \tLoss: 2.3719\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 14.621 \tLoss: 2.3099\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 42.440 \tLoss: 1.5483\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 35.991 \tLoss: 1.7771\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 41.873 \tLoss: 1.5541\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 35.284 \tLoss: 1.7234\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.352 \tLoss: 2.3704\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 41.790 \tLoss: 1.6976\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   7, Avg Accuracy 32.555 | Avg Loss 1.802\n Test: Round   7, Avg Accuracy 30.620 | Avg Loss 1.916\n==========================================================\nidxs_users [ 8 11 12 18 14  4 13  6  3 19 10 15 16  7  1  5  0 17  9  2]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 7 finished in 00:01:11\nEpoch 7 finished. Total time: 571.66 seconds\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 45.563 \tLoss: 1.5035\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 33.075 \tLoss: 1.7540\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.044 \tLoss: 2.3809\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 14.150 \tLoss: 2.4205\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 42.976 \tLoss: 1.5479\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 40.908 \tLoss: 1.6213\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 43.920 \tLoss: 1.5336\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 37.722 \tLoss: 1.7105\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 43.833 \tLoss: 1.5374\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 39.507 \tLoss: 1.6294\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.090 \tLoss: 2.3791\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 12.446 \tLoss: 2.3459\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.358 \tLoss: 2.3703\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 12.453 \tLoss: 2.3643\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 44.501 \tLoss: 1.5274\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 42.161 \tLoss: 1.6625\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round   9, Avg Accuracy 34.144 | Avg Loss 1.767\n Test: Round   9, Avg Accuracy 29.354 | Avg Loss 1.950\n==========================================================\nidxs_users [10  9 13 19 12 15 14 16  0 11  3  2  7  4 18  8  6 17  1  5]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 9 finished in 00:01:10\nEpoch 9 finished. Total time: 713.90 seconds\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 45.673 \tLoss: 1.4806\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 35.264 \tLoss: 2.0659\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 45.450 \tLoss: 1.4965\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 30.590 \tLoss: 1.9580\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 10.177 \tLoss: 2.3656\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 11.456 \tLoss: 2.3702\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 45.694 \tLoss: 1.4819\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 43.979 \tLoss: 1.5371\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 44.076 \tLoss: 1.4859\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 33.466 \tLoss: 1.7641\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.246 \tLoss: 2.3832\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 11.005 \tLoss: 2.4187\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 46.156 \tLoss: 1.4759\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 40.248 \tLoss: 1.7192\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 45.062 \tLoss: 1.4844\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 36.220 \tLoss: 1.6976\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 46.029 \tLoss: 1.4743\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 37.412 \tLoss: 1.7495\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.813 \tLoss: 2.3744\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 13.173 \tLoss: 2.3912\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 43.392 \tLoss: 1.5032\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 41.655 \tLoss: 1.6416\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 9.405 \tLoss: 2.3730\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 9.422 \tLoss: 2.3291\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.324 \tLoss: 2.3726\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 9.617 \tLoss: 2.4377\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 45.427 \tLoss: 1.4868\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 38.328 \tLoss: 1.6485\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 44.145 \tLoss: 1.5069\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 33.270 \tLoss: 2.0585\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 45.977 \tLoss: 1.4703\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 43.238 \tLoss: 1.5221\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 44.030 \tLoss: 1.5031\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 41.756 \tLoss: 1.5813\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 44.901 \tLoss: 1.4836\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 44.504 \tLoss: 1.5665\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.138 \tLoss: 2.3696\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 12.823 \tLoss: 2.3218\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 47.332 \tLoss: 1.4777\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 44.706 \tLoss: 1.6604\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  10, Avg Accuracy 34.722 | Avg Loss 1.752\n Test: Round  10, Avg Accuracy 30.607 | Avg Loss 1.922\n==========================================================\nidxs_users [12 11  2 17  9  3  6  5  4 10  8 14  1 18 19 16 15 13  0  7]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 10 finished in 00:01:11\nEpoch 10 finished. Total time: 785.07 seconds\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 45.942 \tLoss: 1.4846\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 45.447 \tLoss: 1.4472\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 46.245 \tLoss: 1.4897\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 35.143 \tLoss: 1.8515\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 47.114 \tLoss: 1.4575\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 33.829 \tLoss: 1.7131\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 46.905 \tLoss: 1.4593\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 32.038 \tLoss: 2.0245\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.014 \tLoss: 2.3764\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 9.887 \tLoss: 2.3620\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.480 \tLoss: 2.3674\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 11.921 \tLoss: 2.3747\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 46.114 \tLoss: 1.4861\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 40.773 \tLoss: 1.6119\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 46.670 \tLoss: 1.4353\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 38.248 \tLoss: 1.7023\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.099 \tLoss: 2.3722\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 9.247 \tLoss: 2.3891\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.699 \tLoss: 2.3695\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 11.335 \tLoss: 2.4304\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 47.562 \tLoss: 1.4552\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 35.480 \tLoss: 1.8182\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.573 \tLoss: 2.3681\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 10.870 \tLoss: 2.4098\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 46.151 \tLoss: 1.4537\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 37.487 \tLoss: 1.6402\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 48.173 \tLoss: 1.4429\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 50.020 \tLoss: 1.3728\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 47.778 \tLoss: 1.4519\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 32.900 \tLoss: 2.0117\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 45.703 \tLoss: 1.4703\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 41.480 \tLoss: 1.6712\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 45.857 \tLoss: 1.4501\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 38.194 \tLoss: 1.8483\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 46.822 \tLoss: 1.4699\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 38.739 \tLoss: 1.6573\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.338 \tLoss: 2.3689\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 13.039 \tLoss: 2.3117\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 47.443 \tLoss: 1.4435\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 47.171 \tLoss: 1.6307\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  11, Avg Accuracy 35.684 | Avg Loss 1.734\n Test: Round  11, Avg Accuracy 30.662 | Avg Loss 1.914\n==========================================================\nidxs_users [ 7  5 15 12 10 14 19 13  3  1  6  2  9  4 18  8 17 11  0 16]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 11 finished in 00:01:10\nEpoch 11 finished. Total time: 856.02 seconds\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 45.938 \tLoss: 1.4714\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 44.403 \tLoss: 1.5563\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 11.287 \tLoss: 2.3609\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 11.571 \tLoss: 2.4789\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.515 \tLoss: 2.3804\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 14.756 \tLoss: 2.3131\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 46.753 \tLoss: 1.4591\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 43.036 \tLoss: 1.5824\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 46.797 \tLoss: 1.4433\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 38.268 \tLoss: 1.7250\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 45.958 \tLoss: 1.4620\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 47.239 \tLoss: 1.4894\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.871 \tLoss: 2.3701\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 13.524 \tLoss: 2.3242\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 48.649 \tLoss: 1.4132\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 40.147 \tLoss: 1.6678\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 48.330 \tLoss: 1.4040\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 45.312 \tLoss: 1.5457\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 46.949 \tLoss: 1.4303\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 47.946 \tLoss: 1.4471\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 46.648 \tLoss: 1.4336\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 40.207 \tLoss: 1.6172\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 11.585 \tLoss: 2.3721\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 10.338 \tLoss: 2.3759\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 49.115 \tLoss: 1.4269\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 44.679 \tLoss: 1.5865\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 9.855 \tLoss: 2.3749\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 8.405 \tLoss: 2.3732\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 48.212 \tLoss: 1.4458\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 41.245 \tLoss: 1.7701\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.168 \tLoss: 2.3685\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 13.706 \tLoss: 2.4149\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 48.038 \tLoss: 1.4443\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 35.264 \tLoss: 1.7948\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 46.347 \tLoss: 1.4489\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 41.810 \tLoss: 1.5917\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 47.091 \tLoss: 1.4480\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 40.362 \tLoss: 1.6460\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 48.022 \tLoss: 1.4433\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 46.888 \tLoss: 1.6330\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  12, Avg Accuracy 36.356 | Avg Loss 1.720\n Test: Round  12, Avg Accuracy 33.455 | Avg Loss 1.847\n==========================================================\nidxs_users [ 5  2  3  7 11 15  0  9 13  4 18 14 16 10 17  1 12  8 19  6]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 12 finished in 00:01:10\nEpoch 12 finished. Total time: 926.88 seconds\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 48.966 \tLoss: 1.3928\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 41.689 \tLoss: 1.5943\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 47.318 \tLoss: 1.4313\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 42.026 \tLoss: 1.6273\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 9.943 \tLoss: 2.3737\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 13.099 \tLoss: 2.3587\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 47.953 \tLoss: 1.4357\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 38.564 \tLoss: 1.6306\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 11.062 \tLoss: 2.3672\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 11.395 \tLoss: 2.3823\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 46.507 \tLoss: 1.4573\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 37.197 \tLoss: 1.7388\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.159 \tLoss: 2.3702\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 12.884 \tLoss: 2.5326\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 47.969 \tLoss: 1.4234\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 40.834 \tLoss: 1.6734\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 50.820 \tLoss: 1.3979\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 46.538 \tLoss: 1.4787\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 46.489 \tLoss: 1.4415\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 38.268 \tLoss: 1.7315\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 47.181 \tLoss: 1.4293\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 39.137 \tLoss: 1.8714\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 48.706 \tLoss: 1.4111\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 42.693 \tLoss: 1.5731\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 46.753 \tLoss: 1.4193\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 40.066 \tLoss: 1.6804\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 10.276 \tLoss: 2.3676\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 12.042 \tLoss: 2.4319\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 48.955 \tLoss: 1.3999\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 42.706 \tLoss: 1.5458\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 48.876 \tLoss: 1.4096\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 44.545 \tLoss: 1.4862\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 48.658 \tLoss: 1.4213\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 37.412 \tLoss: 1.6924\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.646 \tLoss: 2.3755\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 8.385 \tLoss: 2.4835\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 47.500 \tLoss: 1.4284\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 45.622 \tLoss: 1.6192\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.310 \tLoss: 2.3716\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 41.790 \tLoss: 1.6811\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  13, Avg Accuracy 36.752 | Avg Loss 1.706\n Test: Round  13, Avg Accuracy 33.845 | Avg Loss 1.841\n==========================================================\nidxs_users [17  5  0  8 14 19 10  7 16 15 12 18  9  2 13  4 11  1  6  3]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 13 finished in 00:01:10\nEpoch 13 finished. Total time: 997.76 seconds\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.211 \tLoss: 2.3748\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 9.348 \tLoss: 2.4003\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 46.438 \tLoss: 1.4397\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 37.372 \tLoss: 1.7899\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.896 \tLoss: 2.3634\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 11.005 \tLoss: 2.4090\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 47.137 \tLoss: 1.4246\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 37.958 \tLoss: 1.8296\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 49.267 \tLoss: 1.3711\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 34.011 \tLoss: 2.0220\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 48.614 \tLoss: 1.4175\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 37.864 \tLoss: 1.6370\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 48.208 \tLoss: 1.4157\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 40.544 \tLoss: 1.5567\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 48.208 \tLoss: 1.4017\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 39.386 \tLoss: 1.6431\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 49.864 \tLoss: 1.4046\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 45.077 \tLoss: 1.5728\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 48.433 \tLoss: 1.4016\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 44.956 \tLoss: 1.4688\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 50.611 \tLoss: 1.4044\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 33.055 \tLoss: 1.8998\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 48.876 \tLoss: 1.3952\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 44.060 \tLoss: 1.5200\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.085 \tLoss: 2.3712\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 13.955 \tLoss: 2.3013\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 49.000 \tLoss: 1.4258\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 45.366 \tLoss: 1.6153\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 50.512 \tLoss: 1.3930\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 45.501 \tLoss: 1.5022\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 47.964 \tLoss: 1.4076\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 39.992 \tLoss: 1.9717\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.972 \tLoss: 2.3798\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 10.574 \tLoss: 2.3911\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 11.018 \tLoss: 2.3651\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 8.324 \tLoss: 2.4170\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 11.296 \tLoss: 2.3665\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 13.113 \tLoss: 2.4212\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 48.764 \tLoss: 1.4139\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 47.205 \tLoss: 1.6109\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  14, Avg Accuracy 37.269 | Avg Loss 1.697\n Test: Round  14, Avg Accuracy 31.933 | Avg Loss 1.899\n==========================================================\nidxs_users [10 19  0  5 13 11 12 18  4  8 15 17  3  7 16  9  1 14  2  6]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 14 finished in 00:01:11\nEpoch 14 finished. Total time: 1068.87 seconds\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 9.430 \tLoss: 2.3713\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 11.025 \tLoss: 2.3848\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 49.864 \tLoss: 1.3801\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 42.531 \tLoss: 1.5152\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.124 \tLoss: 2.3748\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 14.817 \tLoss: 2.2995\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 49.184 \tLoss: 1.3795\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 47.050 \tLoss: 1.5917\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 10.333 \tLoss: 2.3789\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 7.664 \tLoss: 2.4592\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 48.665 \tLoss: 1.3953\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 43.784 \tLoss: 1.5101\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 49.361 \tLoss: 1.3662\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 44.504 \tLoss: 1.5941\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 49.580 \tLoss: 1.3822\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 48.485 \tLoss: 1.5573\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 48.941 \tLoss: 1.3957\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 44.114 \tLoss: 1.6052\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 10.159 \tLoss: 2.3794\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 12.958 \tLoss: 2.3290\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 48.141 \tLoss: 1.4000\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 40.955 \tLoss: 1.6966\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 48.847 \tLoss: 1.3945\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 43.002 \tLoss: 1.5381\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 47.856 \tLoss: 1.4050\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 43.454 \tLoss: 1.5657\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 10.071 \tLoss: 2.3773\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 12.783 \tLoss: 2.3090\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 51.059 \tLoss: 1.3625\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 50.451 \tLoss: 1.3740\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 49.800 \tLoss: 1.3771\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 37.742 \tLoss: 1.7913\u001b[00m\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 10.558 \tLoss: 2.3722\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 10.244 \tLoss: 2.4492\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 48.389 \tLoss: 1.4003\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 44.080 \tLoss: 1.6225\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 49.449 \tLoss: 1.4119\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 44.195 \tLoss: 1.5667\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 49.970 \tLoss: 1.3789\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 49.569 \tLoss: 1.6016\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  15, Avg Accuracy 37.489 | Avg Loss 1.684\n Test: Round  15, Avg Accuracy 34.670 | Avg Loss 1.818\n==========================================================\nidxs_users [ 2  8  0 16  1 17  9 18 11  3 15 12  5 14  4 13 10 19  7  6]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 15 finished in 00:01:10\nEpoch 15 finished. Total time: 1139.35 seconds\n\u001b[91m Client10 Train => Local Epoch: 0 \tAcc: 11.477 \tLoss: 2.3624\u001b[00m\n\u001b[92m Client10 Test =>                   \tAcc: 11.342 \tLoss: 2.3506\u001b[00m\n\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 51.914 \tLoss: 1.3604\u001b[00m\n\u001b[92m Client4 Test =>                   \tAcc: 45.131 \tLoss: 1.4788\u001b[00m\n\u001b[91m Client9 Train => Local Epoch: 0 \tAcc: 51.000 \tLoss: 1.3595\u001b[00m\n\u001b[92m Client9 Test =>                   \tAcc: 42.275 \tLoss: 1.5867\u001b[00m\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 10.657 \tLoss: 2.3633\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 10.203 \tLoss: 2.5527\u001b[00m\n\u001b[91m Client19 Train => Local Epoch: 0 \tAcc: 49.157 \tLoss: 1.3891\u001b[00m\n\u001b[92m Client19 Test =>                   \tAcc: 45.508 \tLoss: 1.4882\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 11.625 \tLoss: 2.3586\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 7.759 \tLoss: 2.5373\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.554 \tLoss: 2.3662\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 12.392 \tLoss: 2.3558\u001b[00m\n\u001b[91m Client13 Train => Local Epoch: 0 \tAcc: 51.243 \tLoss: 1.3295\u001b[00m\n\u001b[92m Client13 Test =>                   \tAcc: 37.157 \tLoss: 1.7766\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 50.069 \tLoss: 1.3692\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 44.114 \tLoss: 1.5349\u001b[00m\n\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 9.490 \tLoss: 2.3749\u001b[00m\n\u001b[92m Client3 Test =>                   \tAcc: 10.709 \tLoss: 2.3916\u001b[00m\n\u001b[91m Client6 Train => Local Epoch: 0 \tAcc: 49.600 \tLoss: 1.3815\u001b[00m\n\u001b[92m Client6 Test =>                   \tAcc: 37.473 \tLoss: 1.7914\u001b[00m\n\u001b[91m Client5 Train => Local Epoch: 0 \tAcc: 47.838 \tLoss: 1.3958\u001b[00m\n\u001b[92m Client5 Test =>                   \tAcc: 38.955 \tLoss: 1.6121\u001b[00m\n\u001b[91m Client7 Train => Local Epoch: 0 \tAcc: 50.556 \tLoss: 1.3800\u001b[00m\n\u001b[92m Client7 Test =>                   \tAcc: 41.460 \tLoss: 1.7760\u001b[00m\n\u001b[91m Client11 Train => Local Epoch: 0 \tAcc: 49.343 \tLoss: 1.3743\u001b[00m\n\u001b[92m Client11 Test =>                   \tAcc: 43.804 \tLoss: 1.6258\u001b[00m\n\u001b[91m Client8 Train => Local Epoch: 0 \tAcc: 49.529 \tLoss: 1.3678\u001b[00m\n\u001b[92m Client8 Test =>                   \tAcc: 45.562 \tLoss: 1.4607\u001b[00m\n\u001b[91m Client17 Train => Local Epoch: 0 \tAcc: 50.257 \tLoss: 1.3667\u001b[00m\n\u001b[92m Client17 Test =>                   \tAcc: 43.467 \tLoss: 1.5322\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 51.197 \tLoss: 1.3652\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 34.813 \tLoss: 1.7685\u001b[00m\n\u001b[91m Client16 Train => Local Epoch: 0 \tAcc: 51.199 \tLoss: 1.3585\u001b[00m\n\u001b[92m Client16 Test =>                   \tAcc: 42.221 \tLoss: 1.6872\u001b[00m\n\u001b[91m Client14 Train => Local Epoch: 0 \tAcc: 11.595 \tLoss: 2.3605\u001b[00m\n\u001b[92m Client14 Test =>                   \tAcc: 8.540 \tLoss: 2.3906\u001b[00m\n\u001b[91m Client12 Train => Local Epoch: 0 \tAcc: 49.830 \tLoss: 1.3945\u001b[00m\n\u001b[92m Client12 Test =>                   \tAcc: 44.841 \tLoss: 1.6248\u001b[00m\n------------------------------------------------\n------ Federation process at Server-Side ------- \n------------------------------------------------\n====================== SERVER V1==========================\n Train: Round  16, Avg Accuracy 38.406 | Avg Loss 1.669\n Test: Round  16, Avg Accuracy 32.386 | Avg Loss 1.866\n==========================================================\nidxs_users [10  4  9  2 19  1  0 13 15  3  6  5  7 11  8 17 18 16 14 12]\n------------------------------------------------------------\n------ Fed Server: Federation process at Client-Side -------\n------------------------------------------------------------\nEpoch 16 finished in 00:01:10\nEpoch 16 finished. Total time: 1209.91 seconds\n\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 11.179 \tLoss: 2.3656\u001b[00m\n\u001b[92m Client2 Test =>                   \tAcc: 12.392 \tLoss: 2.3362\u001b[00m\n\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 9.972 \tLoss: 2.3713\u001b[00m\n\u001b[92m Client1 Test =>                   \tAcc: 8.331 \tLoss: 2.4747\u001b[00m\n\u001b[91m Client15 Train => Local Epoch: 0 \tAcc: 49.795 \tLoss: 1.3723\u001b[00m\n\u001b[92m Client15 Test =>                   \tAcc: 41.029 \tLoss: 1.6009\u001b[00m\n\u001b[91m Client18 Train => Local Epoch: 0 \tAcc: 50.308 \tLoss: 1.3409\u001b[00m\n\u001b[92m Client18 Test =>                   \tAcc: 34.853 \tLoss: 1.9671\u001b[00m\n\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 10.147 \tLoss: 2.3689\u001b[00m\n\u001b[92m Client0 Test =>                   \tAcc: 13.820 \tLoss: 2.3245\u001b[00m\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Training and Evaluation completed!\")\n\n# ===============================================================================\n# Save output data to .excel file (we use for comparision plots)\nround_process = [i for i in range(1, len(acc_train_collect) + 1)]\ndf = DataFrame({'round': round_process, 'acc_train': acc_train_collect, 'acc_test': acc_test_collect,\n                'loss_train': loss_train_collect, 'loss_test': loss_test_collect})\nfile_name = program + \"_\" + dataset_choice + \".xlsx\"\ndf.to_excel(file_name, sheet_name=\"v1_test\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_train_collect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 创建一个名为df的DataFrame\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'country': ['USA', 'Canada', 'Australia']\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}